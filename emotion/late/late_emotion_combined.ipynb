{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1Ebbwa1VfFBd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "import torch.nn.functional as F\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from sklearn.metrics import f1_score, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rV94bO7MfMZl"
      },
      "outputs": [],
      "source": [
        "class VisionTextConversationDataset(Dataset):\n",
        "    def __init__(self, data_dir, embeddings_dir):\n",
        "        self.data_dir = data_dir\n",
        "        self.embeddings_dir = embeddings_dir\n",
        "        self.conversations = self.load_conversations()\n",
        "        self.emotion_list = sorted(['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness','surprise'])\n",
        "        self.encoder = LabelEncoder()\n",
        "        self.encoder.fit(self.emotion_list)\n",
        "        # print(f\"Initialized ConversationDataset with {len(self.conversations)} conversations\")\n",
        "\n",
        "\n",
        "    def encode_emotion(self, emotion):\n",
        "        # return self.encoder.transform([emotion])[0]\n",
        "        encoded = self.encoder.transform([emotion])[0]\n",
        "        # print(f\"Encoded from here'{emotion}' to {encoded}\")\n",
        "        return encoded\n",
        "\n",
        "    def load_conversations(self):\n",
        "        with open(self.data_dir, 'r') as file:\n",
        "            conversations = json.load(file)\n",
        "        # print(f\"Loaded {len(conversations)} conversations\")\n",
        "        return conversations\n",
        "\n",
        "    def load_embeddings(self, video_name):\n",
        "        video_name = video_name.split('.')[0]\n",
        "\n",
        "        embedding_file = os.path.join(self.embeddings_dir, f'{video_name}.npy')\n",
        "        embedding = np.load(embedding_file)\n",
        "        # print(f\"Loaded embeddings from {embedding_file}\")\n",
        "        return embedding\n",
        "\n",
        "    def positional_encoding(self, embeddings):\n",
        "        seq_length = embeddings.shape[0]\n",
        "        embedding_dim = embeddings.shape[1]\n",
        "        position_enc = torch.zeros(seq_length, embedding_dim)\n",
        "        position = torch.arange(0, seq_length, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-np.log(10000.0) / embedding_dim))\n",
        "        position_enc[:, 0::2] = torch.sin(position * div_term)\n",
        "        position_enc[:, 1::2] = torch.cos(position * div_term)\n",
        "        # print(position_enc.shape)\n",
        "        # print(\"Generated positional encoding\")\n",
        "        return position_enc\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.conversations)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        conversation = self.conversations[idx]\n",
        "        context_embeddings = []\n",
        "        emotions = []\n",
        "        # print(conversation)\n",
        "        conversation_ID = conversation['conversation_ID']\n",
        "        utterance_list = []\n",
        "        labels_list = []\n",
        "        # print(conversation_ID)\n",
        "\n",
        "        for utterance in conversation['conversation']:\n",
        "            video_name = utterance['video_name']\n",
        "            utterance_ID = utterance['utterance_ID']\n",
        "            utterance_list.append(utterance_ID)\n",
        "            emotion = self.encode_emotion(utterance['emotion'])\n",
        "            labels_list.append(emotion)\n",
        "            embedding = self.load_embeddings(video_name)\n",
        "            if len(embedding.shape) == 1:\n",
        "              embedding = embedding.reshape(1, -1)  # Reshape to (1, 768)\n",
        "            context_embeddings.append(torch.from_numpy(embedding))\n",
        "            emotions.append(emotion)\n",
        "\n",
        "        max_seq_length = 33\n",
        "        utterance_list = utterance_list + [0] * (33 - len(utterance_list)) if len(utterance_list) < 33 else utterance_list\n",
        "        labels_list = labels_list + [-1] * (33 - len(labels_list)) if len(labels_list) < 33 else labels_list\n",
        "\n",
        "        padded_embeddings = []\n",
        "        num_to_add = 0\n",
        "        if len(context_embeddings) < max_seq_length:\n",
        "            num_to_add = max_seq_length - len(context_embeddings)\n",
        "            zero_tensor = torch.zeros((1,768), dtype=torch.float32)\n",
        "            context_embeddings += [zero_tensor] * num_to_add\n",
        "\n",
        "        # Check the dimensions of tensors in context_embeddings\n",
        "        # for i, tensor in enumerate(context_embeddings):\n",
        "        #     print(f\"Tensor {i+1} shape:\", tensor.shape)\n",
        "\n",
        "        context_embeddings_padded = torch.cat(context_embeddings, dim=0)\n",
        "\n",
        "        positional_encodings = self.positional_encoding(context_embeddings_padded)\n",
        "        context_embeddings_with_pos = context_embeddings_padded + positional_encodings\n",
        "\n",
        "        # emotions += [self.encode_emotion('neutral')] * num_to_add\n",
        "        emotions += [-1] * num_to_add\n",
        "        encoded_emotions_tensor = torch.tensor(emotions, dtype=torch.long)\n",
        "\n",
        "        # print(f\"Returning data for conversation ID {conversation_ID}\")\n",
        "        # print(f\"Utterances in conversation {conversation_ID}: {utterance_list}\")\n",
        "        # print(f\"Emotions in conversation {conversation_ID}: {labels_list}\")\n",
        "        return context_embeddings_with_pos, encoded_emotions_tensor, torch.tensor(conversation_ID), utterance_list, labels_list\n",
        "        # return context_embeddings_with_pos, encoded_emotions_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WDCKfCTy2O5R"
      },
      "outputs": [],
      "source": [
        "class AudioConversationDataset(Dataset):\n",
        "    def __init__(self, data_dir, embeddings_dir):\n",
        "        self.data_dir = data_dir\n",
        "        self.embeddings_dir = embeddings_dir\n",
        "        self.conversations = self.load_conversations()\n",
        "        self.emotion_list = sorted(['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness','surprise'])\n",
        "        self.encoder = LabelEncoder()\n",
        "        self.encoder.fit(self.emotion_list)\n",
        "        # print(f\"Initialized ConversationDataset with {len(self.conversations)} conversations\")\n",
        "\n",
        "\n",
        "    def encode_emotion(self, emotion):\n",
        "        # return self.encoder.transform([emotion])[0]\n",
        "        encoded = self.encoder.transform([emotion])[0]\n",
        "        # print(f\"Encoded from here'{emotion}' to {encoded}\")\n",
        "        return encoded\n",
        "\n",
        "    def load_conversations(self):\n",
        "        with open(self.data_dir, 'r') as file:\n",
        "            conversations = json.load(file)\n",
        "        # print(f\"Loaded {len(conversations)} conversations\")\n",
        "        return conversations\n",
        "\n",
        "    def load_embeddings(self, video_name):\n",
        "        video_name = video_name.split('.')[0]\n",
        "\n",
        "        embedding_file = os.path.join(self.embeddings_dir, f'{video_name}.npy')\n",
        "        embedding = np.load(embedding_file)\n",
        "        # print(f\"Loaded embeddings from {embedding_file}\")\n",
        "        return embedding\n",
        "\n",
        "    def positional_encoding(self, embeddings):\n",
        "        seq_length = embeddings.shape[0]\n",
        "        embedding_dim = embeddings.shape[1]\n",
        "        position_enc = torch.zeros(seq_length, embedding_dim)\n",
        "        position = torch.arange(0, seq_length, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-np.log(10000.0) / embedding_dim))\n",
        "        position_enc[:, 0::2] = torch.sin(position * div_term)\n",
        "        position_enc[:, 1::2] = torch.cos(position * div_term)\n",
        "        # print(position_enc.shape)\n",
        "        # print(\"Generated positional encoding\")\n",
        "        return position_enc\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.conversations)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        conversation = self.conversations[idx]\n",
        "        context_embeddings = []\n",
        "        emotions = []\n",
        "        # print(conversation)\n",
        "        conversation_ID = conversation['conversation_ID']\n",
        "        utterance_list = []\n",
        "        labels_list = []\n",
        "        # print(conversation_ID)\n",
        "\n",
        "        for utterance in conversation['conversation']:\n",
        "            video_name = utterance['video_name']\n",
        "            utterance_ID = utterance['utterance_ID']\n",
        "            utterance_list.append(utterance_ID)\n",
        "            emotion = self.encode_emotion(utterance['emotion'])\n",
        "            labels_list.append(emotion)\n",
        "            embedding = self.load_embeddings(video_name)\n",
        "            context_embeddings.append(torch.from_numpy(embedding))\n",
        "            emotions.append(emotion)\n",
        "\n",
        "        max_seq_length = 33\n",
        "        utterance_list = utterance_list + [0] * (33 - len(utterance_list)) if len(utterance_list) < 33 else utterance_list\n",
        "        labels_list = labels_list + [-1] * (33 - len(labels_list)) if len(labels_list) < 33 else labels_list\n",
        "\n",
        "        padded_embeddings = []\n",
        "        num_to_add = 0\n",
        "        if len(context_embeddings) < max_seq_length:\n",
        "            num_to_add = max_seq_length - len(context_embeddings)\n",
        "            zero_tensor = torch.zeros((1,1024), dtype=torch.float32)\n",
        "            context_embeddings += [zero_tensor] * num_to_add\n",
        "\n",
        "        # Check the dimensions of tensors in context_embeddings\n",
        "        # for i, tensor in enumerate(context_embeddings):\n",
        "        #     print(f\"Tensor {i+1} shape:\", tensor.shape)\n",
        "\n",
        "        context_embeddings_padded = torch.cat(context_embeddings, dim=0)\n",
        "\n",
        "        positional_encodings = self.positional_encoding(context_embeddings_padded)\n",
        "        context_embeddings_with_pos = context_embeddings_padded + positional_encodings\n",
        "\n",
        "        # emotions += [self.encode_emotion('neutral')] * num_to_add\n",
        "        emotions += [-1] * num_to_add\n",
        "        encoded_emotions_tensor = torch.tensor(emotions, dtype=torch.long)\n",
        "\n",
        "        # print(f\"Returning data for conversation ID {conversation_ID}\")\n",
        "        # print(f\"Utterances in conversation {conversation_ID}: {utterance_list}\")\n",
        "        # print(f\"Emotions in conversation {conversation_ID}: {labels_list}\")\n",
        "        return context_embeddings_with_pos, encoded_emotions_tensor, torch.tensor(conversation_ID), utterance_list, labels_list\n",
        "        # return context_embeddings_with_pos, encoded_emotions_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QEkJcGJEfc04"
      },
      "outputs": [],
      "source": [
        "class EmotionDetector(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_emotions, n_layers=2, dropout=0.2):\n",
        "        super(EmotionDetector, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_emotions = num_emotions\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        encoder_layers = TransformerEncoderLayer(d_model=input_dim, nhead=4, dim_feedforward=hidden_dim, dropout=dropout, batch_first=True)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers=n_layers)\n",
        "\n",
        "        self.decoder_emotion = nn.Linear(input_dim, num_emotions)\n",
        "        # print(\"Initialized EmotionDetector\")\n",
        "\n",
        "    def forward(self, context_embeddings):\n",
        "        encoded_context = self.transformer_encoder(context_embeddings)\n",
        "        prediction_emotion = self.decoder_emotion(encoded_context)  # (batch_size, seq_length, num_emotions)\n",
        "        # print(\"Generated predictions\")\n",
        "        return prediction_emotion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_tQzSd4Blmr-"
      },
      "outputs": [],
      "source": [
        "vision_model = EmotionDetector(input_dim=768,\n",
        "                        hidden_dim=512,\n",
        "                        num_emotions=7,\n",
        "                        n_layers=4)\n",
        "\n",
        "audio_model = EmotionDetector(input_dim=1024,\n",
        "                        hidden_dim=512,\n",
        "                        num_emotions=7,\n",
        "                        n_layers=4)\n",
        "\n",
        "text_model = EmotionDetector(input_dim=768,\n",
        "                        hidden_dim=512,\n",
        "                        num_emotions=7,\n",
        "                        n_layers=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_n7C3Uil_MJ",
        "outputId": "5d7c9ca4-3233-4688-8dba-79c6419bd94f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vision_model.load_state_dict(torch.load('/project/msoleyma_1026/ecp/models/emotion/late/emotion_detection_model_late_video.pt'))\n",
        "audio_model.load_state_dict(torch.load('/project/msoleyma_1026/ecp/models/emotion/late/emotion_detection_model_late_audio.pt'))\n",
        "text_model.load_state_dict(torch.load('/project/msoleyma_1026/ecp/models/emotion/late/emotion_detection_model_late_text.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6-TIzp8LjUyc"
      },
      "outputs": [],
      "source": [
        "# def evaluate_model(model, dataloader, criterion, device, num_emotions):\n",
        "#     \"\"\"\n",
        "#     Evaluate the model on the given dataloader and compute loss and accuracy, accounting for padding.\n",
        "\n",
        "#     Args:\n",
        "#     - model (torch.nn.Module): The model to evaluate.\n",
        "#     - dataloader (DataLoader): The DataLoader providing the dataset.\n",
        "#     - criterion (loss function): The loss function used to evaluate the model's performance.\n",
        "#     - device (torch.device): The device computations will be performed on.\n",
        "#     - num_emotions (int): Number of emotion categories used in the model output.\n",
        "\n",
        "#     Returns:\n",
        "#     - float: Average loss over the dataset.\n",
        "#     - float: Accuracy, excluding padded data points.\n",
        "#     \"\"\"\n",
        "#     model.eval()\n",
        "#     total_loss = 0.0\n",
        "#     total_correct = 0\n",
        "#     total_valid = 0  # Total non-padded data points\n",
        "#     predictions = []\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         for batch in dataloader:\n",
        "#             context_embeddings, emotions, conversationList, utteranceList, labelsList = batch\n",
        "#             context_embeddings, emotions = context_embeddings.to(device), emotions.to(device)\n",
        "\n",
        "#             outputs = model(context_embeddings)\n",
        "\n",
        "#             # Reshape for loss calculation\n",
        "#             outputs_reshaped = outputs.view(-1, num_emotions)\n",
        "#             emotions_reshaped = emotions.view(-1)\n",
        "\n",
        "#             # Compute the loss only on non-padded data points\n",
        "#             active_outputs = outputs_reshaped[emotions_reshaped != -1]\n",
        "#             active_emotions = emotions_reshaped[emotions_reshaped != -1]\n",
        "#             loss = criterion(active_outputs, active_emotions)\n",
        "#             total_loss += loss.item()\n",
        "\n",
        "#             # Calculate accuracy\n",
        "#             _, predicted = torch.max(outputs, dim=2)\n",
        "#             valid_data = (emotions != -1)\n",
        "#             correct_predictions = predicted.eq(emotions) & valid_data\n",
        "#             total_correct += correct_predictions.sum().item()\n",
        "#             total_valid += valid_data.sum().item()\n",
        "\n",
        "#             for i, conv_id in enumerate(conversationList):\n",
        "#                 conv_data = {'conversation_id': int(conv_id), 'utterances': []}\n",
        "#                 for j, utterance_tensor in enumerate(utteranceList):\n",
        "#                     if int(utterance_tensor[i]) != 0:\n",
        "#                         conv_data['utterances'].append({'utt_id': int(utterance_tensor[i]), 'emotion': int(predicted[i][j])})\n",
        "#                 predictions.append(conv_data)\n",
        "\n",
        "#     file_path = \"/project/msoleyma_1026/ecp/data/predictions/late_fusion_predictions_text.json\"\n",
        "\n",
        "#     with open(file_path, 'w') as file:\n",
        "#         json.dump(predictions, file, indent=4)\n",
        "\n",
        "#     average_loss = total_loss / len(dataloader)\n",
        "#     accuracy = total_correct / total_valid if total_valid > 0 else 0\n",
        "#     return average_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "UGVmCo4pEZFn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "def late_evaluate_model(text_model, video_model, audio_model, text_loader, video_loader, audio_loader, criterion, device, num_emotions):\n",
        "    \"\"\"\n",
        "    Evaluate the model on the given dataloaders for text, video, and audio modalities and compute loss and accuracy, accounting for padding.\n",
        "\n",
        "    Args:\n",
        "    - model (torch.nn.Module): The model to evaluate.\n",
        "    - text_loader (DataLoader): The DataLoader providing the text dataset.\n",
        "    - video_loader (DataLoader): The DataLoader providing the video dataset.\n",
        "    - audio_loader (DataLoader): The DataLoader providing the audio dataset.\n",
        "    - criterion (loss function): The loss function used to evaluate the model's performance.\n",
        "    - device (torch.device): The device computations will be performed on.\n",
        "    - num_emotions (int): Number of emotion categories used in the model output.\n",
        "\n",
        "    Returns:\n",
        "    - float: Average loss over the dataset.\n",
        "    - float: Accuracy, excluding padded data points.\n",
        "    \"\"\"\n",
        "    # model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_valid = 0  # Total non-padded data points\n",
        "    all_predictions = []\n",
        "    all_true_emotions = []\n",
        "    predictions = []\n",
        "\n",
        "    with torch.no_grad(), tqdm(total=len(text_loader)) as pbar:\n",
        "        for (context_embeddings_text, emotions_text, conversation_ID_text, utteranceList, _), (context_embeddings_video, emotions_video, conversation_ID_video, utteranceList, _), (context_embeddings_audio, emotions_audio, conversation_ID_audio, utteranceList, _) in zip(text_loader, video_loader, audio_loader):\n",
        "            # Move tensors to device\n",
        "            pbar.update(1)\n",
        "            context_embeddings_text, emotions_text = context_embeddings_text.to(device), emotions_text.to(device)\n",
        "            context_embeddings_video, emotions_video = context_embeddings_video.to(device), emotions_video.to(device)\n",
        "            context_embeddings_audio, emotions_audio = context_embeddings_audio.to(device), emotions_audio.to(device)\n",
        "\n",
        "            # Pass the context embeddings through the models to get predictions\n",
        "            outputs_text = text_model(context_embeddings_text)\n",
        "            outputs_video = video_model(context_embeddings_video)\n",
        "            outputs_audio = audio_model(context_embeddings_audio)\n",
        "\n",
        "            # Combine predictions from different modalities\n",
        "            combined_outputs = (outputs_text + outputs_video + outputs_audio) / 3.0\n",
        "\n",
        "            # Reshape for loss calculation\n",
        "            outputs_reshaped = combined_outputs.view(-1, num_emotions)\n",
        "            emotions_reshaped = emotions_text.view(-1)\n",
        "\n",
        "            # Compute the loss only on non-padded data points\n",
        "            active_outputs = outputs_reshaped[emotions_reshaped != -1]\n",
        "            active_emotions = emotions_reshaped[emotions_reshaped != -1]\n",
        "            loss = criterion(active_outputs, active_emotions)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            _, predicted = torch.max(combined_outputs, dim=2)\n",
        "            valid_data = (emotions_text != -1)\n",
        "            correct_predictions = predicted.eq(emotions_text) & valid_data\n",
        "            total_correct += correct_predictions.sum().item()\n",
        "            total_valid += valid_data.sum().item()\n",
        "\n",
        "            all_predictions.extend(predicted[valid_data].cpu().numpy())\n",
        "            all_true_emotions.extend(active_emotions.cpu().numpy())\n",
        "\n",
        "            for i, conv_id in enumerate(conversation_ID_text):\n",
        "                conv_data = {'conversation_id': int(conv_id), 'utterances': []}\n",
        "                for j, utterance_tensor in enumerate(utteranceList):\n",
        "                    if int(utterance_tensor[i]) != 0:\n",
        "                        conv_data['utterances'].append({'utt_id': int(utterance_tensor[i]), 'emotion': int(predicted[i][j])})\n",
        "                predictions.append(conv_data)\n",
        "\n",
        "    file_path = \"/project/msoleyma_1026/ecp/data/predictions/late_fusion_predictions_all.json\"\n",
        "\n",
        "    with open(file_path, 'w') as file:\n",
        "        json.dump(predictions, file, indent=4)\n",
        "\n",
        "    f1_score_val = f1_score(all_true_emotions, all_predictions, average='weighted')\n",
        "    classification_report_result = classification_report(all_true_emotions, all_predictions)\n",
        "\n",
        "    average_loss = total_loss / len(text_loader)\n",
        "    accuracy = total_correct / total_valid if total_valid > 0 else 0\n",
        "\n",
        "    return average_loss, accuracy, f1_score_val, classification_report_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCZGZ_d6nIlh",
        "outputId": "71e48d95-406d-4a2c-ec2c-f66c808ec188"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "EmotionDetector(\n",
              "  (transformer_encoder): TransformerEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-3): 4 x TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=768, out_features=512, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "        (linear2): Linear(in_features=512, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.2, inplace=False)\n",
              "        (dropout2): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (decoder_emotion): Linear(in_features=768, out_features=7, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_dir_test = \"/project/msoleyma_1026/ecp/data/test.json\"\n",
        "\n",
        "vision_embeddings_dir_test = \"/project/msoleyma_1026/ecp/data/video/test/\"\n",
        "audio_embeddings_dir_test = \"/project/msoleyma_1026/ecp/data/audio/test-emotion/\"\n",
        "text_embeddings_dir_test = \"/project/msoleyma_1026/ecp/data/text/test-emotion/\"\n",
        "\n",
        "vision_dataset = VisionTextConversationDataset(data_dir_test, vision_embeddings_dir_test)\n",
        "text_dataset = VisionTextConversationDataset(data_dir_test, text_embeddings_dir_test)\n",
        "audio_dataset = AudioConversationDataset(data_dir_test, audio_embeddings_dir_test)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "vision_test_loader = DataLoader(vision_dataset, batch_size=8, shuffle=False)\n",
        "audio_test_loader = DataLoader(audio_dataset, batch_size=8, shuffle=False)\n",
        "text_test_loader = DataLoader(text_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "vision_model.to(device)\n",
        "audio_model.to(device)\n",
        "text_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QO1aH_kyp7Cg",
        "outputId": "cb789bdd-bc31-40b8-e1c7-cc4da8f6132a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "EmotionDetector(\n",
              "  (transformer_encoder): TransformerEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-3): 4 x TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=768, out_features=512, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "        (linear2): Linear(in_features=512, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.2, inplace=False)\n",
              "        (dropout2): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (decoder_emotion): Linear(in_features=768, out_features=7, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_predictions = []\n",
        "\n",
        "vision_model.eval()\n",
        "audio_model.eval()\n",
        "text_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "wZFD6j4aDNDh"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 33/33 [02:15<00:00,  4.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss: 1.2967, Validation Accuracy: 0.5409%\n",
            "F1 Score: 45.99%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.20      0.30       333\n",
            "           1       0.00      0.00      0.00        79\n",
            "           2       0.00      0.00      0.00        56\n",
            "           3       0.59      0.32      0.42       429\n",
            "           4       0.52      0.97      0.68      1121\n",
            "           5       0.53      0.03      0.06       241\n",
            "           6       0.74      0.28      0.41       307\n",
            "\n",
            "    accuracy                           0.54      2566\n",
            "   macro avg       0.43      0.26      0.27      2566\n",
            "weighted avg       0.54      0.54      0.46      2566\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "/project/msoleyma_1026/ecp/ecp-venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/project/msoleyma_1026/ecp/ecp-venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/project/msoleyma_1026/ecp/ecp-venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion_emotion = nn.CrossEntropyLoss()\n",
        "\n",
        "val_loss, val_accuracy, val_f1, val_class_report = late_evaluate_model(text_model, vision_model, audio_model, text_test_loader, vision_test_loader, audio_test_loader, criterion_emotion, device, 7)\n",
        "print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}%\")\n",
        "print(f\"F1 Score: {val_f1 * 100:.2f}%\")\n",
        "print(val_class_report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZqqQDEMv2NA"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            