{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "sQgNYU5ub1qg",
        "outputId": "a6291b16-9e16-4581-a80b-85601b076057"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, 768)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "x = np.load(\"/project/msoleyma_1026/ecp/data/text/train-emotion/dia1utt1.npy\")\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "WWi29yesQ_VI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "import torch.nn.functional as F\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from sklearn.metrics import f1_score, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-ebSo_3zAfqz"
      },
      "outputs": [],
      "source": [
        "class ConversationDataset(Dataset):\n",
        "    def __init__(self, data_dir, embeddings_dir):\n",
        "        self.data_dir = data_dir\n",
        "        self.embeddings_dir = embeddings_dir\n",
        "        self.conversations = self.load_conversations()\n",
        "        self.emotion_list = sorted(['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness','surprise'])\n",
        "        self.encoder = LabelEncoder()\n",
        "        self.encoder.fit(self.emotion_list)\n",
        "\n",
        "\n",
        "    def encode_emotion(self, emotion):\n",
        "        encoded = self.encoder.transform([emotion])[0]\n",
        "        return encoded\n",
        "\n",
        "    def load_conversations(self):\n",
        "        with open(self.data_dir, 'r') as file:\n",
        "            conversations = json.load(file)\n",
        "        # print(f\"Loaded {len(conversations)} conversations\")\n",
        "        return conversations\n",
        "\n",
        "    def load_embeddings(self, video_name):\n",
        "        video_name = video_name.split('.')[0]\n",
        "\n",
        "        embedding_file = os.path.join(self.embeddings_dir, f'{video_name}.npy')\n",
        "        embedding = np.load(embedding_file)\n",
        "        return embedding\n",
        "\n",
        "    def positional_encoding(self, embeddings):\n",
        "        seq_length = embeddings.shape[0]\n",
        "        embedding_dim = embeddings.shape[1]\n",
        "        position_enc = torch.zeros(seq_length, embedding_dim)\n",
        "        position = torch.arange(0, seq_length, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-np.log(10000.0) / embedding_dim))\n",
        "        position_enc[:, 0::2] = torch.sin(position * div_term)\n",
        "        position_enc[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        return position_enc\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.conversations)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        conversation = self.conversations[idx]\n",
        "        context_embeddings = []\n",
        "        emotions = []\n",
        "        conversation_ID = conversation['conversation_ID']\n",
        "        utterance_list = []\n",
        "        labels_list = []\n",
        "\n",
        "        for utterance in conversation['conversation']:\n",
        "            video_name = utterance['video_name']\n",
        "            utterance_ID = utterance['utterance_ID']\n",
        "            utterance_list.append(utterance_ID)\n",
        "            emotion = self.encode_emotion(utterance['emotion'])\n",
        "            labels_list.append(emotion)\n",
        "            embedding = self.load_embeddings(video_name)\n",
        "            context_embeddings.append(torch.from_numpy(embedding))\n",
        "            emotions.append(emotion)\n",
        "\n",
        "        max_seq_length = 33\n",
        "        utterance_list = utterance_list + [0] * (33 - len(utterance_list)) if len(utterance_list) < 33 else utterance_list\n",
        "        labels_list = labels_list + [-1] * (33 - len(labels_list)) if len(labels_list) < 33 else labels_list\n",
        "\n",
        "        padded_embeddings = []\n",
        "        num_to_add = 0\n",
        "        if len(context_embeddings) < max_seq_length:\n",
        "            num_to_add = max_seq_length - len(context_embeddings)\n",
        "            zero_tensor = torch.zeros((1,768), dtype=torch.float32)\n",
        "            context_embeddings += [zero_tensor] * num_to_add\n",
        "\n",
        "        context_embeddings_padded = torch.cat(context_embeddings, dim=0)\n",
        "\n",
        "        positional_encodings = self.positional_encoding(context_embeddings_padded)\n",
        "        context_embeddings_with_pos = context_embeddings_padded + positional_encodings\n",
        "\n",
        "\n",
        "        emotions += [-1] * num_to_add\n",
        "        encoded_emotions_tensor = torch.tensor(emotions, dtype=torch.long)\n",
        "\n",
        "\n",
        "        return context_embeddings_with_pos, encoded_emotions_tensor, torch.tensor(conversation_ID), utterance_list, labels_list\n",
        "\n",
        "class EmotionDetector(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_emotions, n_layers=2, dropout=0.2):\n",
        "        super(EmotionDetector, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_emotions = num_emotions\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        encoder_layers = TransformerEncoderLayer(d_model=input_dim, nhead=4, dim_feedforward=hidden_dim, dropout=dropout, batch_first=True)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers=n_layers)\n",
        "\n",
        "        self.decoder_emotion = nn.Linear(input_dim, num_emotions)\n",
        "        print(\"Initialized EmotionDetector\")\n",
        "\n",
        "    def forward(self, context_embeddings):\n",
        "        encoded_context = self.transformer_encoder(context_embeddings)\n",
        "        prediction_emotion = self.decoder_emotion(encoded_context)  # (batch_size, seq_length, num_emotions)\n",
        "        return prediction_emotion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_e9CkeiQfekH",
        "outputId": "f10fe488-a2da-4d2d-fe76-44e4ebba7c5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleared CUDA cache\n",
            "Using device: cuda\n",
            "Initialized EmotionDetector\n",
            "Epoch [1/30], Batch [5/18], Emotion Loss: 1.6611\n",
            "Epoch [1/30], Batch [10/18], Emotion Loss: 1.5264\n",
            "Epoch [1/30], Batch [15/18], Emotion Loss: 1.5940\n",
            "Epoch [1/30], Average Emotion Loss: 1.6567\n",
            "Epoch [2/30], Batch [5/18], Emotion Loss: 1.6466\n",
            "Epoch [2/30], Batch [10/18], Emotion Loss: 1.5897\n",
            "Epoch [2/30], Batch [15/18], Emotion Loss: 1.4985\n",
            "Epoch [2/30], Average Emotion Loss: 1.5733\n",
            "Epoch [3/30], Batch [5/18], Emotion Loss: 1.5528\n",
            "Epoch [3/30], Batch [10/18], Emotion Loss: 1.5622\n",
            "Epoch [3/30], Batch [15/18], Emotion Loss: 1.4856\n",
            "Epoch [3/30], Average Emotion Loss: 1.5337\n",
            "Epoch [4/30], Batch [5/18], Emotion Loss: 1.4999\n",
            "Epoch [4/30], Batch [10/18], Emotion Loss: 1.4538\n",
            "Epoch [4/30], Batch [15/18], Emotion Loss: 1.4781\n",
            "Epoch [4/30], Average Emotion Loss: 1.4853\n",
            "Epoch [5/30], Batch [5/18], Emotion Loss: 1.4054\n",
            "Epoch [5/30], Batch [10/18], Emotion Loss: 1.3997\n",
            "Epoch [5/30], Batch [15/18], Emotion Loss: 1.5295\n",
            "Epoch [5/30], Average Emotion Loss: 1.4289\n",
            "Epoch [6/30], Batch [5/18], Emotion Loss: 1.4626\n",
            "Epoch [6/30], Batch [10/18], Emotion Loss: 1.3498\n",
            "Epoch [6/30], Batch [15/18], Emotion Loss: 1.4452\n",
            "Epoch [6/30], Average Emotion Loss: 1.3788\n",
            "Epoch [7/30], Batch [5/18], Emotion Loss: 1.2729\n",
            "Epoch [7/30], Batch [10/18], Emotion Loss: 1.2846\n",
            "Epoch [7/30], Batch [15/18], Emotion Loss: 1.2840\n",
            "Epoch [7/30], Average Emotion Loss: 1.3389\n",
            "Epoch [8/30], Batch [5/18], Emotion Loss: 1.3689\n",
            "Epoch [8/30], Batch [10/18], Emotion Loss: 1.2754\n",
            "Epoch [8/30], Batch [15/18], Emotion Loss: 1.3484\n",
            "Epoch [8/30], Average Emotion Loss: 1.2973\n",
            "Epoch [9/30], Batch [5/18], Emotion Loss: 1.3094\n",
            "Epoch [9/30], Batch [10/18], Emotion Loss: 1.3273\n",
            "Epoch [9/30], Batch [15/18], Emotion Loss: 1.3892\n",
            "Epoch [9/30], Average Emotion Loss: 1.2656\n",
            "Epoch [10/30], Batch [5/18], Emotion Loss: 1.2844\n",
            "Epoch [10/30], Batch [10/18], Emotion Loss: 1.3200\n",
            "Epoch [10/30], Batch [15/18], Emotion Loss: 1.2279\n",
            "Epoch [10/30], Average Emotion Loss: 1.2513\n",
            "Epoch [11/30], Batch [5/18], Emotion Loss: 1.2394\n",
            "Epoch [11/30], Batch [10/18], Emotion Loss: 1.2728\n",
            "Epoch [11/30], Batch [15/18], Emotion Loss: 1.2788\n",
            "Epoch [11/30], Average Emotion Loss: 1.2373\n",
            "Epoch [12/30], Batch [5/18], Emotion Loss: 1.2702\n",
            "Epoch [12/30], Batch [10/18], Emotion Loss: 1.2449\n",
            "Epoch [12/30], Batch [15/18], Emotion Loss: 1.1531\n",
            "Epoch [12/30], Average Emotion Loss: 1.2302\n",
            "Epoch [13/30], Batch [5/18], Emotion Loss: 1.2035\n",
            "Epoch [13/30], Batch [10/18], Emotion Loss: 1.2949\n",
            "Epoch [13/30], Batch [15/18], Emotion Loss: 1.2238\n",
            "Epoch [13/30], Average Emotion Loss: 1.2115\n",
            "Epoch [14/30], Batch [5/18], Emotion Loss: 1.2413\n",
            "Epoch [14/30], Batch [10/18], Emotion Loss: 1.2463\n",
            "Epoch [14/30], Batch [15/18], Emotion Loss: 1.0714\n",
            "Epoch [14/30], Average Emotion Loss: 1.2050\n",
            "Epoch [15/30], Batch [5/18], Emotion Loss: 1.2420\n",
            "Epoch [15/30], Batch [10/18], Emotion Loss: 1.2239\n",
            "Epoch [15/30], Batch [15/18], Emotion Loss: 1.0835\n",
            "Epoch [15/30], Average Emotion Loss: 1.1964\n",
            "Epoch [16/30], Batch [5/18], Emotion Loss: 1.1332\n",
            "Epoch [16/30], Batch [10/18], Emotion Loss: 1.1374\n",
            "Epoch [16/30], Batch [15/18], Emotion Loss: 1.1989\n",
            "Epoch [16/30], Average Emotion Loss: 1.1841\n",
            "Epoch [17/30], Batch [5/18], Emotion Loss: 1.3082\n",
            "Epoch [17/30], Batch [10/18], Emotion Loss: 1.1528\n",
            "Epoch [17/30], Batch [15/18], Emotion Loss: 1.1379\n",
            "Epoch [17/30], Average Emotion Loss: 1.1811\n",
            "Epoch [18/30], Batch [5/18], Emotion Loss: 1.1996\n",
            "Epoch [18/30], Batch [10/18], Emotion Loss: 1.1995\n",
            "Epoch [18/30], Batch [15/18], Emotion Loss: 1.1691\n",
            "Epoch [18/30], Average Emotion Loss: 1.1815\n",
            "Epoch [19/30], Batch [5/18], Emotion Loss: 1.1416\n",
            "Epoch [19/30], Batch [10/18], Emotion Loss: 1.2114\n",
            "Epoch [19/30], Batch [15/18], Emotion Loss: 1.1554\n",
            "Epoch [19/30], Average Emotion Loss: 1.1746\n",
            "Epoch [20/30], Batch [5/18], Emotion Loss: 1.0734\n",
            "Epoch [20/30], Batch [10/18], Emotion Loss: 1.2593\n",
            "Epoch [20/30], Batch [15/18], Emotion Loss: 1.1568\n",
            "Epoch [20/30], Average Emotion Loss: 1.1557\n",
            "Epoch [21/30], Batch [5/18], Emotion Loss: 1.1646\n",
            "Epoch [21/30], Batch [10/18], Emotion Loss: 1.2151\n",
            "Epoch [21/30], Batch [15/18], Emotion Loss: 1.1625\n",
            "Epoch [21/30], Average Emotion Loss: 1.1566\n",
            "Epoch [22/30], Batch [5/18], Emotion Loss: 1.1630\n",
            "Epoch [22/30], Batch [10/18], Emotion Loss: 1.1563\n",
            "Epoch [22/30], Batch [15/18], Emotion Loss: 1.1319\n",
            "Epoch [22/30], Average Emotion Loss: 1.1464\n",
            "Epoch [23/30], Batch [5/18], Emotion Loss: 1.1090\n",
            "Epoch [23/30], Batch [10/18], Emotion Loss: 1.1932\n",
            "Epoch [23/30], Batch [15/18], Emotion Loss: 1.1904\n",
            "Epoch [23/30], Average Emotion Loss: 1.1374\n",
            "Epoch [24/30], Batch [5/18], Emotion Loss: 1.1639\n",
            "Epoch [24/30], Batch [10/18], Emotion Loss: 1.1624\n",
            "Epoch [24/30], Batch [15/18], Emotion Loss: 1.1210\n",
            "Epoch [24/30], Average Emotion Loss: 1.1385\n",
            "Epoch [25/30], Batch [5/18], Emotion Loss: 0.9786\n",
            "Epoch [25/30], Batch [10/18], Emotion Loss: 1.1325\n",
            "Epoch [25/30], Batch [15/18], Emotion Loss: 1.1598\n",
            "Epoch [25/30], Average Emotion Loss: 1.1254\n",
            "Epoch [26/30], Batch [5/18], Emotion Loss: 1.1740\n",
            "Epoch [26/30], Batch [10/18], Emotion Loss: 1.0870\n",
            "Epoch [26/30], Batch [15/18], Emotion Loss: 1.0839\n",
            "Epoch [26/30], Average Emotion Loss: 1.1195\n",
            "Epoch [27/30], Batch [5/18], Emotion Loss: 1.1659\n",
            "Epoch [27/30], Batch [10/18], Emotion Loss: 1.0424\n",
            "Epoch [27/30], Batch [15/18], Emotion Loss: 1.1433\n",
            "Epoch [27/30], Average Emotion Loss: 1.1239\n",
            "Epoch [28/30], Batch [5/18], Emotion Loss: 1.1758\n",
            "Epoch [28/30], Batch [10/18], Emotion Loss: 0.9758\n",
            "Epoch [28/30], Batch [15/18], Emotion Loss: 1.0611\n",
            "Epoch [28/30], Average Emotion Loss: 1.1210\n",
            "Epoch [29/30], Batch [5/18], Emotion Loss: 1.0493\n",
            "Epoch [29/30], Batch [10/18], Emotion Loss: 1.0691\n",
            "Epoch [29/30], Batch [15/18], Emotion Loss: 1.2027\n",
            "Epoch [29/30], Average Emotion Loss: 1.1073\n",
            "Epoch [30/30], Batch [5/18], Emotion Loss: 1.0723\n",
            "Epoch [30/30], Batch [10/18], Emotion Loss: 1.0411\n",
            "Epoch [30/30], Batch [15/18], Emotion Loss: 1.0838\n",
            "Epoch [30/30], Average Emotion Loss: 1.1071\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"Cleared CUDA cache\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "data_dir = \"/project/msoleyma_1026/ecp/data/train.json\"\n",
        "embeddings_dir = \"/project/msoleyma_1026/ecp/data/text/train-emotion/\"\n",
        "num_emotions = 7  # Number of emotions for classification\n",
        "input_dim = 768  # Dimensionality of your embeddings\n",
        "hidden_dim = 512  # Hidden dimension for the Transformer\n",
        "n_layers = 4  # Number of layers in the Transformer\n",
        "dropout = 0.2  # Dropout probability\n",
        "batch_size = 64\n",
        "num_epochs = 30\n",
        "learning_rate = 1e-5\n",
        "print_interval = 5\n",
        "\n",
        "dataset = ConversationDataset(data_dir, embeddings_dir)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "model = EmotionDetector(input_dim, hidden_dim, num_emotions, n_layers=n_layers, dropout=dropout).to(device)\n",
        "\n",
        "criterion_emotion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss_emotion = 0.0\n",
        "    for batch_idx, (context_embeddings, emotions, convID, utteranceList, labelsList) in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        context_embeddings, emotions = context_embeddings.to(device), emotions.to(device)\n",
        "        prediction_emotion_logits = model(context_embeddings)\n",
        "        \n",
        "        outputs_reshaped = prediction_emotion_logits.view(-1, num_emotions)\n",
        "        emotions_reshaped = emotions.view(-1)\n",
        "\n",
        "        # Compute the loss only on non-padded data points\n",
        "        active_outputs = outputs_reshaped[emotions_reshaped != -1]\n",
        "        active_emotions = emotions_reshaped[emotions_reshaped != -1]\n",
        "        loss_emotion = criterion_emotion(active_outputs, active_emotions)\n",
        "        \n",
        "        loss_emotion.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss_emotion += loss_emotion.item()\n",
        "\n",
        "        if (batch_idx + 1) % print_interval == 0:\n",
        "            print(f'Epoch [{epoch + 1}/{num_epochs}], Batch [{batch_idx + 1}/{len(dataloader)}], '\n",
        "                  f'Emotion Loss: {loss_emotion.item():.4f}')\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}], Average Emotion Loss: {epoch_loss_emotion / len(dataloader):.4f}')\n",
        "\n",
        "torch.save(model.state_dict(), '/project/msoleyma_1026/ecp/models/emotion/late/emotion_detection_model_late_text_emotion.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6-ogDmuZ6Yz",
        "outputId": "45f9babe-292c-4cf6-c1e0-e627982f7ac0"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, dataloader, criterion, device, num_emotions):\n",
        "    \"\"\"\n",
        "    Evaluate the model on the given dataloader and compute loss and accuracy, accounting for padding.\n",
        "\n",
        "    Args:\n",
        "    - model (torch.nn.Module): The model to evaluate.\n",
        "    - dataloader (DataLoader): The DataLoader providing the dataset.\n",
        "    - criterion (loss function): The loss function used to evaluate the model's performance.\n",
        "    - device (torch.device): The device computations will be performed on.\n",
        "    - num_emotions (int): Number of emotion categories used in the model output.\n",
        "\n",
        "    Returns:\n",
        "    - float: Average loss over the dataset.\n",
        "    - float: Accuracy, excluding padded data points.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_valid = 0  # Total non-padded data points\n",
        "    all_predictions = []\n",
        "    all_true_emotions = []\n",
        "    predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            context_embeddings, emotions, conversationList, utteranceList, labelsList = batch\n",
        "            context_embeddings, emotions = context_embeddings.to(device), emotions.to(device)\n",
        "\n",
        "            outputs = model(context_embeddings)\n",
        "\n",
        "            # Reshape for loss calculation\n",
        "            outputs_reshaped = outputs.view(-1, num_emotions)\n",
        "            emotions_reshaped = emotions.view(-1)\n",
        "\n",
        "            # Compute the loss only on non-padded data points\n",
        "            active_outputs = outputs_reshaped[emotions_reshaped != -1]\n",
        "            active_emotions = emotions_reshaped[emotions_reshaped != -1]\n",
        "            loss = criterion(active_outputs, active_emotions)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            _, predicted = torch.max(outputs, dim=2)\n",
        "            valid_data = (emotions != -1)\n",
        "            correct_predictions = predicted.eq(emotions) & valid_data\n",
        "            total_correct += correct_predictions.sum().item()\n",
        "            total_valid += valid_data.sum().item()\n",
        "\n",
        "            all_predictions.extend(predicted[valid_data].cpu().numpy())\n",
        "            all_true_emotions.extend(active_emotions.cpu().numpy())\n",
        "\n",
        "            for i, conv_id in enumerate(conversationList):\n",
        "                conv_data = {'conversation_id': int(conv_id), 'utterances': []}\n",
        "                for j, utterance_tensor in enumerate(utteranceList):\n",
        "                    if int(utterance_tensor[i]) != 0:\n",
        "                        conv_data['utterances'].append({'utt_id': int(utterance_tensor[i]), 'emotion': int(predicted[i][j])})\n",
        "                predictions.append(conv_data)\n",
        "\n",
        "    file_path = \"/project/msoleyma_1026/ecp/data/predictions/late_fusion_predictions_text_emotion.json\"\n",
        "\n",
        "    with open(file_path, 'w') as file:\n",
        "        json.dump(predictions, file, indent=4)\n",
        "\n",
        "    f1_score_val = f1_score(all_true_emotions, all_predictions, average='weighted')\n",
        "    classification_report_result = classification_report(all_true_emotions, all_predictions)\n",
        "\n",
        "    average_loss = total_loss / len(dataloader)\n",
        "    accuracy = total_correct / total_valid if total_valid > 0 else 0\n",
        "\n",
        "    return average_loss, accuracy, f1_score_val, classification_report_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss: 1.2047, Validation Accuracy: 0.5814%\n",
            "F1 Score: 55.65%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.45      0.40      0.42       333\n",
            "           1       0.00      0.00      0.00        79\n",
            "           2       0.21      0.05      0.09        56\n",
            "           3       0.53      0.55      0.54       429\n",
            "           4       0.65      0.80      0.72      1121\n",
            "           5       0.39      0.28      0.32       241\n",
            "           6       0.58      0.50      0.54       307\n",
            "\n",
            "    accuracy                           0.58      2566\n",
            "   macro avg       0.40      0.37      0.38      2566\n",
            "weighted avg       0.54      0.58      0.56      2566\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/project/msoleyma_1026/ecp/ecp-venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/project/msoleyma_1026/ecp/ecp-venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/project/msoleyma_1026/ecp/ecp-venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "data_dir_test = \"/project/msoleyma_1026/ecp/data/test.json\"\n",
        "embeddings_dir_test = \"/project/msoleyma_1026/ecp/data/text/test-emotion/\"\n",
        "\n",
        "dataset = ConversationDataset(data_dir_test, embeddings_dir_test)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "validation_dataloader = DataLoader(dataset, batch_size=8, shuffle=False)  # Same dataloader for simplicity\n",
        "model.to(device)\n",
        "criterion_emotion = nn.CrossEntropyLoss()\n",
        "\n",
        "val_loss, val_accuracy, val_f1, val_class_report = evaluate_model(model, validation_dataloader, criterion_emotion, device, num_emotions=7)\n",
        "print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}%\")\n",
        "print(f\"F1 Score: {val_f1 * 100:.2f}%\")\n",
        "print(val_class_report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     