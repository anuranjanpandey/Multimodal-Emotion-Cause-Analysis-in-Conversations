{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQgNYU5ub1qg",
        "outputId": "652e24ee-f19f-4a2e-f41b-27700e923889"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, 768)"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "x = np.load(\"/project/msoleyma_1026/ecp/data/video/train/dia360utt19.npy\")\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WWi29yesQ_VI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "import torch.nn.functional as F\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from sklearn.metrics import f1_score, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-ebSo_3zAfqz"
      },
      "outputs": [],
      "source": [
        "class ConversationDataset(Dataset):\n",
        "    def __init__(self, data_dir, embeddings_dir):\n",
        "        self.data_dir = data_dir\n",
        "        self.embeddings_dir = embeddings_dir\n",
        "        self.conversations = self.load_conversations()\n",
        "        self.emotion_list = sorted(['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness','surprise'])\n",
        "        self.encoder = LabelEncoder()\n",
        "        self.encoder.fit(self.emotion_list)\n",
        "\n",
        "\n",
        "    def encode_emotion(self, emotion):\n",
        "        encoded = self.encoder.transform([emotion])[0]\n",
        "        return encoded\n",
        "\n",
        "    def load_conversations(self):\n",
        "        with open(self.data_dir, 'r') as file:\n",
        "            conversations = json.load(file)\n",
        "        # print(f\"Loaded {len(conversations)} conversations\")\n",
        "        return conversations\n",
        "\n",
        "    def load_embeddings(self, video_name):\n",
        "        video_name = video_name.split('.')[0]\n",
        "\n",
        "        embedding_file = os.path.join(self.embeddings_dir, f'{video_name}.npy')\n",
        "        # print(embedding_file)\n",
        "        embedding = np.load(embedding_file)\n",
        "        return embedding\n",
        "\n",
        "    def positional_encoding(self, embeddings):\n",
        "        seq_length = embeddings.shape[0]\n",
        "        embedding_dim = embeddings.shape[1]\n",
        "        position_enc = torch.zeros(seq_length, embedding_dim)\n",
        "        position = torch.arange(0, seq_length, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-np.log(10000.0) / embedding_dim))\n",
        "        position_enc[:, 0::2] = torch.sin(position * div_term)\n",
        "        position_enc[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        return position_enc\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.conversations)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        conversation = self.conversations[idx]\n",
        "        context_embeddings = []\n",
        "        emotions = []\n",
        "        conversation_ID = conversation['conversation_ID']\n",
        "        utterance_list = []\n",
        "        labels_list = []\n",
        "\n",
        "        for utterance in conversation['conversation']:\n",
        "            video_name = utterance['video_name']\n",
        "            utterance_ID = utterance['utterance_ID']\n",
        "            utterance_list.append(utterance_ID)\n",
        "            emotion = self.encode_emotion(utterance['emotion'])\n",
        "            labels_list.append(emotion)\n",
        "            embedding = self.load_embeddings(video_name)\n",
        "            # Reshape the embedding if it's of shape torch.Size([768])\n",
        "            if len(embedding.shape) == 1:  # Check if embedding is 1D\n",
        "              embedding = embedding.reshape(1, -1)  # Reshape to (1, 768)\n",
        "            context_embeddings.append(torch.from_numpy(embedding))\n",
        "            emotions.append(emotion)\n",
        "\n",
        "        max_seq_length = 33\n",
        "        utterance_list = utterance_list + [0] * (33 - len(utterance_list)) if len(utterance_list) < 33 else utterance_list\n",
        "        labels_list = labels_list + [-1] * (33 - len(labels_list)) if len(labels_list) < 33 else labels_list\n",
        "\n",
        "        padded_embeddings = []\n",
        "        num_to_add = 0\n",
        "        if len(context_embeddings) < max_seq_length:\n",
        "            num_to_add = max_seq_length - len(context_embeddings)\n",
        "            zero_tensor = torch.zeros((1,768), dtype=torch.float32)\n",
        "            context_embeddings += [zero_tensor] * num_to_add\n",
        "\n",
        "        context_embeddings_padded = torch.cat(context_embeddings, dim=0)\n",
        "\n",
        "        positional_encodings = self.positional_encoding(context_embeddings_padded)\n",
        "        context_embeddings_with_pos = context_embeddings_padded + positional_encodings\n",
        "\n",
        "\n",
        "        emotions += [-1] * num_to_add\n",
        "        encoded_emotions_tensor = torch.tensor(emotions, dtype=torch.long)\n",
        "\n",
        "\n",
        "        return context_embeddings_with_pos, encoded_emotions_tensor, torch.tensor(conversation_ID), utterance_list, labels_list\n",
        "\n",
        "class EmotionDetector(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_emotions, n_layers=2, dropout=0.2):\n",
        "        super(EmotionDetector, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_emotions = num_emotions\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        encoder_layers = TransformerEncoderLayer(d_model=input_dim, nhead=4, dim_feedforward=hidden_dim, dropout=dropout, batch_first=True)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers=n_layers)\n",
        "\n",
        "        self.decoder_emotion = nn.Linear(input_dim, num_emotions)\n",
        "        print(\"Initialized EmotionDetector\")\n",
        "\n",
        "    def forward(self, context_embeddings):\n",
        "        encoded_context = self.transformer_encoder(context_embeddings)\n",
        "        prediction_emotion = self.decoder_emotion(encoded_context)  # (batch_size, seq_length, num_emotions)\n",
        "        return prediction_emotion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_e9CkeiQfekH",
        "outputId": "9321620e-8e78-4377-86bb-6163dc13377e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleared CUDA cache\n",
            "Using device: cuda\n",
            "Initialized EmotionDetector\n",
            "Epoch [1/30], Batch [5/35], Emotion Loss: 1.4860\n",
            "Epoch [1/30], Batch [10/35], Emotion Loss: 1.6346\n",
            "Epoch [1/30], Batch [15/35], Emotion Loss: 1.6067\n",
            "Epoch [1/30], Batch [20/35], Emotion Loss: 1.6609\n",
            "Epoch [1/30], Batch [25/35], Emotion Loss: 1.5750\n",
            "Epoch [1/30], Batch [30/35], Emotion Loss: 1.6352\n",
            "Epoch [1/30], Batch [35/35], Emotion Loss: 1.6780\n",
            "Epoch [1/30], Average Emotion Loss: 1.6565\n",
            "Epoch [2/30], Batch [5/35], Emotion Loss: 1.6518\n",
            "Epoch [2/30], Batch [10/35], Emotion Loss: 1.5725\n",
            "Epoch [2/30], Batch [15/35], Emotion Loss: 1.5291\n",
            "Epoch [2/30], Batch [20/35], Emotion Loss: 1.6183\n",
            "Epoch [2/30], Batch [25/35], Emotion Loss: 1.6562\n",
            "Epoch [2/30], Batch [30/35], Emotion Loss: 1.6924\n",
            "Epoch [2/30], Batch [35/35], Emotion Loss: 1.6474\n",
            "Epoch [2/30], Average Emotion Loss: 1.6136\n",
            "Epoch [3/30], Batch [5/35], Emotion Loss: 1.5894\n",
            "Epoch [3/30], Batch [10/35], Emotion Loss: 1.6145\n",
            "Epoch [3/30], Batch [15/35], Emotion Loss: 1.6086\n",
            "Epoch [3/30], Batch [20/35], Emotion Loss: 1.5943\n",
            "Epoch [3/30], Batch [25/35], Emotion Loss: 1.6465\n",
            "Epoch [3/30], Batch [30/35], Emotion Loss: 1.6582\n",
            "Epoch [3/30], Batch [35/35], Emotion Loss: 1.5906\n",
            "Epoch [3/30], Average Emotion Loss: 1.6150\n",
            "Epoch [4/30], Batch [5/35], Emotion Loss: 1.6012\n",
            "Epoch [4/30], Batch [10/35], Emotion Loss: 1.6080\n",
            "Epoch [4/30], Batch [15/35], Emotion Loss: 1.7433\n",
            "Epoch [4/30], Batch [20/35], Emotion Loss: 1.5081\n",
            "Epoch [4/30], Batch [25/35], Emotion Loss: 1.7095\n",
            "Epoch [4/30], Batch [30/35], Emotion Loss: 1.6939\n",
            "Epoch [4/30], Batch [35/35], Emotion Loss: 1.7169\n",
            "Epoch [4/30], Average Emotion Loss: 1.6120\n",
            "Epoch [5/30], Batch [5/35], Emotion Loss: 1.5490\n",
            "Epoch [5/30], Batch [10/35], Emotion Loss: 1.5819\n",
            "Epoch [5/30], Batch [15/35], Emotion Loss: 1.7036\n",
            "Epoch [5/30], Batch [20/35], Emotion Loss: 1.6904\n",
            "Epoch [5/30], Batch [25/35], Emotion Loss: 1.4806\n",
            "Epoch [5/30], Batch [30/35], Emotion Loss: 1.5809\n",
            "Epoch [5/30], Batch [35/35], Emotion Loss: 1.5939\n",
            "Epoch [5/30], Average Emotion Loss: 1.6094\n",
            "Epoch [6/30], Batch [5/35], Emotion Loss: 1.5829\n",
            "Epoch [6/30], Batch [10/35], Emotion Loss: 1.5523\n",
            "Epoch [6/30], Batch [15/35], Emotion Loss: 1.6124\n",
            "Epoch [6/30], Batch [20/35], Emotion Loss: 1.6365\n",
            "Epoch [6/30], Batch [25/35], Emotion Loss: 1.4877\n",
            "Epoch [6/30], Batch [30/35], Emotion Loss: 1.6309\n",
            "Epoch [6/30], Batch [35/35], Emotion Loss: 1.4552\n",
            "Epoch [6/30], Average Emotion Loss: 1.6094\n",
            "Epoch [7/30], Batch [5/35], Emotion Loss: 1.6151\n",
            "Epoch [7/30], Batch [10/35], Emotion Loss: 1.7844\n",
            "Epoch [7/30], Batch [15/35], Emotion Loss: 1.5478\n",
            "Epoch [7/30], Batch [20/35], Emotion Loss: 1.5797\n",
            "Epoch [7/30], Batch [25/35], Emotion Loss: 1.5967\n",
            "Epoch [7/30], Batch [30/35], Emotion Loss: 1.6394\n",
            "Epoch [7/30], Batch [35/35], Emotion Loss: 1.5603\n",
            "Epoch [7/30], Average Emotion Loss: 1.6065\n",
            "Epoch [8/30], Batch [5/35], Emotion Loss: 1.6012\n",
            "Epoch [8/30], Batch [10/35], Emotion Loss: 1.7322\n",
            "Epoch [8/30], Batch [15/35], Emotion Loss: 1.5428\n",
            "Epoch [8/30], Batch [20/35], Emotion Loss: 1.5700\n",
            "Epoch [8/30], Batch [25/35], Emotion Loss: 1.5424\n",
            "Epoch [8/30], Batch [30/35], Emotion Loss: 1.6278\n",
            "Epoch [8/30], Batch [35/35], Emotion Loss: 1.6819\n",
            "Epoch [8/30], Average Emotion Loss: 1.6033\n",
            "Epoch [9/30], Batch [5/35], Emotion Loss: 1.5514\n",
            "Epoch [9/30], Batch [10/35], Emotion Loss: 1.5336\n",
            "Epoch [9/30], Batch [15/35], Emotion Loss: 1.6228\n",
            "Epoch [9/30], Batch [20/35], Emotion Loss: 1.6451\n",
            "Epoch [9/30], Batch [25/35], Emotion Loss: 1.5499\n",
            "Epoch [9/30], Batch [30/35], Emotion Loss: 1.5848\n",
            "Epoch [9/30], Batch [35/35], Emotion Loss: 1.5990\n",
            "Epoch [9/30], Average Emotion Loss: 1.5987\n",
            "Epoch [10/30], Batch [5/35], Emotion Loss: 1.6154\n",
            "Epoch [10/30], Batch [10/35], Emotion Loss: 1.5995\n",
            "Epoch [10/30], Batch [15/35], Emotion Loss: 1.6190\n",
            "Epoch [10/30], Batch [20/35], Emotion Loss: 1.6918\n",
            "Epoch [10/30], Batch [25/35], Emotion Loss: 1.6327\n",
            "Epoch [10/30], Batch [30/35], Emotion Loss: 1.7074\n",
            "Epoch [10/30], Batch [35/35], Emotion Loss: 1.6769\n",
            "Epoch [10/30], Average Emotion Loss: 1.6038\n",
            "Epoch [11/30], Batch [5/35], Emotion Loss: 1.7042\n",
            "Epoch [11/30], Batch [10/35], Emotion Loss: 1.5799\n",
            "Epoch [11/30], Batch [15/35], Emotion Loss: 1.6686\n",
            "Epoch [11/30], Batch [20/35], Emotion Loss: 1.5766\n",
            "Epoch [11/30], Batch [25/35], Emotion Loss: 1.5867\n",
            "Epoch [11/30], Batch [30/35], Emotion Loss: 1.4952\n",
            "Epoch [11/30], Batch [35/35], Emotion Loss: 1.5521\n",
            "Epoch [11/30], Average Emotion Loss: 1.6047\n",
            "Epoch [12/30], Batch [5/35], Emotion Loss: 1.6496\n",
            "Epoch [12/30], Batch [10/35], Emotion Loss: 1.6416\n",
            "Epoch [12/30], Batch [15/35], Emotion Loss: 1.6281\n",
            "Epoch [12/30], Batch [20/35], Emotion Loss: 1.6784\n",
            "Epoch [12/30], Batch [25/35], Emotion Loss: 1.5233\n",
            "Epoch [12/30], Batch [30/35], Emotion Loss: 1.6840\n",
            "Epoch [12/30], Batch [35/35], Emotion Loss: 1.5926\n",
            "Epoch [12/30], Average Emotion Loss: 1.5978\n",
            "Epoch [13/30], Batch [5/35], Emotion Loss: 1.5840\n",
            "Epoch [13/30], Batch [10/35], Emotion Loss: 1.6123\n",
            "Epoch [13/30], Batch [15/35], Emotion Loss: 1.7220\n",
            "Epoch [13/30], Batch [20/35], Emotion Loss: 1.4369\n",
            "Epoch [13/30], Batch [25/35], Emotion Loss: 1.4589\n",
            "Epoch [13/30], Batch [30/35], Emotion Loss: 1.6429\n",
            "Epoch [13/30], Batch [35/35], Emotion Loss: 1.6366\n",
            "Epoch [13/30], Average Emotion Loss: 1.6044\n",
            "Epoch [14/30], Batch [5/35], Emotion Loss: 1.5000\n",
            "Epoch [14/30], Batch [10/35], Emotion Loss: 1.5627\n",
            "Epoch [14/30], Batch [15/35], Emotion Loss: 1.5566\n",
            "Epoch [14/30], Batch [20/35], Emotion Loss: 1.6702\n",
            "Epoch [14/30], Batch [25/35], Emotion Loss: 1.5864\n",
            "Epoch [14/30], Batch [30/35], Emotion Loss: 1.6641\n",
            "Epoch [14/30], Batch [35/35], Emotion Loss: 1.6844\n",
            "Epoch [14/30], Average Emotion Loss: 1.6005\n",
            "Epoch [15/30], Batch [5/35], Emotion Loss: 1.6163\n",
            "Epoch [15/30], Batch [10/35], Emotion Loss: 1.6080\n",
            "Epoch [15/30], Batch [15/35], Emotion Loss: 1.5189\n",
            "Epoch [15/30], Batch [20/35], Emotion Loss: 1.5290\n",
            "Epoch [15/30], Batch [25/35], Emotion Loss: 1.6556\n",
            "Epoch [15/30], Batch [30/35], Emotion Loss: 1.5146\n",
            "Epoch [15/30], Batch [35/35], Emotion Loss: 1.6942\n",
            "Epoch [15/30], Average Emotion Loss: 1.6029\n",
            "Epoch [16/30], Batch [5/35], Emotion Loss: 1.5849\n",
            "Epoch [16/30], Batch [10/35], Emotion Loss: 1.6644\n",
            "Epoch [16/30], Batch [15/35], Emotion Loss: 1.5805\n",
            "Epoch [16/30], Batch [20/35], Emotion Loss: 1.5319\n",
            "Epoch [16/30], Batch [25/35], Emotion Loss: 1.6327\n",
            "Epoch [16/30], Batch [30/35], Emotion Loss: 1.5142\n",
            "Epoch [16/30], Batch [35/35], Emotion Loss: 1.5609\n",
            "Epoch [16/30], Average Emotion Loss: 1.5995\n",
            "Epoch [17/30], Batch [5/35], Emotion Loss: 1.7492\n",
            "Epoch [17/30], Batch [10/35], Emotion Loss: 1.6668\n",
            "Epoch [17/30], Batch [15/35], Emotion Loss: 1.5800\n",
            "Epoch [17/30], Batch [20/35], Emotion Loss: 1.5764\n",
            "Epoch [17/30], Batch [25/35], Emotion Loss: 1.5462\n",
            "Epoch [17/30], Batch [30/35], Emotion Loss: 1.5678\n",
            "Epoch [17/30], Batch [35/35], Emotion Loss: 1.5395\n",
            "Epoch [17/30], Average Emotion Loss: 1.5980\n",
            "Epoch [18/30], Batch [5/35], Emotion Loss: 1.5808\n",
            "Epoch [18/30], Batch [10/35], Emotion Loss: 1.6909\n",
            "Epoch [18/30], Batch [15/35], Emotion Loss: 1.6111\n",
            "Epoch [18/30], Batch [20/35], Emotion Loss: 1.5487\n",
            "Epoch [18/30], Batch [25/35], Emotion Loss: 1.6637\n",
            "Epoch [18/30], Batch [30/35], Emotion Loss: 1.5852\n",
            "Epoch [18/30], Batch [35/35], Emotion Loss: 1.5632\n",
            "Epoch [18/30], Average Emotion Loss: 1.5996\n",
            "Epoch [19/30], Batch [5/35], Emotion Loss: 1.6152\n",
            "Epoch [19/30], Batch [10/35], Emotion Loss: 1.6397\n",
            "Epoch [19/30], Batch [15/35], Emotion Loss: 1.6028\n",
            "Epoch [19/30], Batch [20/35], Emotion Loss: 1.6710\n",
            "Epoch [19/30], Batch [25/35], Emotion Loss: 1.5686\n",
            "Epoch [19/30], Batch [30/35], Emotion Loss: 1.5937\n",
            "Epoch [19/30], Batch [35/35], Emotion Loss: 1.6682\n",
            "Epoch [19/30], Average Emotion Loss: 1.5979\n",
            "Epoch [20/30], Batch [5/35], Emotion Loss: 1.6120\n",
            "Epoch [20/30], Batch [10/35], Emotion Loss: 1.7331\n",
            "Epoch [20/30], Batch [15/35], Emotion Loss: 1.6041\n",
            "Epoch [20/30], Batch [20/35], Emotion Loss: 1.5601\n",
            "Epoch [20/30], Batch [25/35], Emotion Loss: 1.6140\n",
            "Epoch [20/30], Batch [30/35], Emotion Loss: 1.5848\n",
            "Epoch [20/30], Batch [35/35], Emotion Loss: 1.6147\n",
            "Epoch [20/30], Average Emotion Loss: 1.5961\n",
            "Epoch [21/30], Batch [5/35], Emotion Loss: 1.6634\n",
            "Epoch [21/30], Batch [10/35], Emotion Loss: 1.6685\n",
            "Epoch [21/30], Batch [15/35], Emotion Loss: 1.5502\n",
            "Epoch [21/30], Batch [20/35], Emotion Loss: 1.6352\n",
            "Epoch [21/30], Batch [25/35], Emotion Loss: 1.5646\n",
            "Epoch [21/30], Batch [30/35], Emotion Loss: 1.5556\n",
            "Epoch [21/30], Batch [35/35], Emotion Loss: 1.7459\n",
            "Epoch [21/30], Average Emotion Loss: 1.6042\n",
            "Epoch [22/30], Batch [5/35], Emotion Loss: 1.5275\n",
            "Epoch [22/30], Batch [10/35], Emotion Loss: 1.5938\n",
            "Epoch [22/30], Batch [15/35], Emotion Loss: 1.5779\n",
            "Epoch [22/30], Batch [20/35], Emotion Loss: 1.5212\n",
            "Epoch [22/30], Batch [25/35], Emotion Loss: 1.6502\n",
            "Epoch [22/30], Batch [30/35], Emotion Loss: 1.6705\n",
            "Epoch [22/30], Batch [35/35], Emotion Loss: 1.5967\n",
            "Epoch [22/30], Average Emotion Loss: 1.5928\n",
            "Epoch [23/30], Batch [5/35], Emotion Loss: 1.5965\n",
            "Epoch [23/30], Batch [10/35], Emotion Loss: 1.6129\n",
            "Epoch [23/30], Batch [15/35], Emotion Loss: 1.5835\n",
            "Epoch [23/30], Batch [20/35], Emotion Loss: 1.5453\n",
            "Epoch [23/30], Batch [25/35], Emotion Loss: 1.6934\n",
            "Epoch [23/30], Batch [30/35], Emotion Loss: 1.5925\n",
            "Epoch [23/30], Batch [35/35], Emotion Loss: 1.5801\n",
            "Epoch [23/30], Average Emotion Loss: 1.5971\n",
            "Epoch [24/30], Batch [5/35], Emotion Loss: 1.6702\n",
            "Epoch [24/30], Batch [10/35], Emotion Loss: 1.5953\n",
            "Epoch [24/30], Batch [15/35], Emotion Loss: 1.5730\n",
            "Epoch [24/30], Batch [20/35], Emotion Loss: 1.6505\n",
            "Epoch [24/30], Batch [25/35], Emotion Loss: 1.4784\n",
            "Epoch [24/30], Batch [30/35], Emotion Loss: 1.6004\n",
            "Epoch [24/30], Batch [35/35], Emotion Loss: 1.5677\n",
            "Epoch [24/30], Average Emotion Loss: 1.5934\n",
            "Epoch [25/30], Batch [5/35], Emotion Loss: 1.6351\n",
            "Epoch [25/30], Batch [10/35], Emotion Loss: 1.7376\n",
            "Epoch [25/30], Batch [15/35], Emotion Loss: 1.5726\n",
            "Epoch [25/30], Batch [20/35], Emotion Loss: 1.6184\n",
            "Epoch [25/30], Batch [25/35], Emotion Loss: 1.5471\n",
            "Epoch [25/30], Batch [30/35], Emotion Loss: 1.6080\n",
            "Epoch [25/30], Batch [35/35], Emotion Loss: 1.5946\n",
            "Epoch [25/30], Average Emotion Loss: 1.5920\n",
            "Epoch [26/30], Batch [5/35], Emotion Loss: 1.6258\n",
            "Epoch [26/30], Batch [10/35], Emotion Loss: 1.5777\n",
            "Epoch [26/30], Batch [15/35], Emotion Loss: 1.6085\n",
            "Epoch [26/30], Batch [20/35], Emotion Loss: 1.6668\n",
            "Epoch [26/30], Batch [25/35], Emotion Loss: 1.5673\n",
            "Epoch [26/30], Batch [30/35], Emotion Loss: 1.4736\n",
            "Epoch [26/30], Batch [35/35], Emotion Loss: 1.5645\n",
            "Epoch [26/30], Average Emotion Loss: 1.5882\n",
            "Epoch [27/30], Batch [5/35], Emotion Loss: 1.4365\n",
            "Epoch [27/30], Batch [10/35], Emotion Loss: 1.6381\n",
            "Epoch [27/30], Batch [15/35], Emotion Loss: 1.5233\n",
            "Epoch [27/30], Batch [20/35], Emotion Loss: 1.4646\n",
            "Epoch [27/30], Batch [25/35], Emotion Loss: 1.5543\n",
            "Epoch [27/30], Batch [30/35], Emotion Loss: 1.5924\n",
            "Epoch [27/30], Batch [35/35], Emotion Loss: 1.5383\n",
            "Epoch [27/30], Average Emotion Loss: 1.5910\n",
            "Epoch [28/30], Batch [5/35], Emotion Loss: 1.6376\n",
            "Epoch [28/30], Batch [10/35], Emotion Loss: 1.5388\n",
            "Epoch [28/30], Batch [15/35], Emotion Loss: 1.6620\n",
            "Epoch [28/30], Batch [20/35], Emotion Loss: 1.6152\n",
            "Epoch [28/30], Batch [25/35], Emotion Loss: 1.5805\n",
            "Epoch [28/30], Batch [30/35], Emotion Loss: 1.6601\n",
            "Epoch [28/30], Batch [35/35], Emotion Loss: 1.5548\n",
            "Epoch [28/30], Average Emotion Loss: 1.5920\n",
            "Epoch [29/30], Batch [5/35], Emotion Loss: 1.5919\n",
            "Epoch [29/30], Batch [10/35], Emotion Loss: 1.5778\n",
            "Epoch [29/30], Batch [15/35], Emotion Loss: 1.4982\n",
            "Epoch [29/30], Batch [20/35], Emotion Loss: 1.6116\n",
            "Epoch [29/30], Batch [25/35], Emotion Loss: 1.5706\n",
            "Epoch [29/30], Batch [30/35], Emotion Loss: 1.5238\n",
            "Epoch [29/30], Batch [35/35], Emotion Loss: 1.5831\n",
            "Epoch [29/30], Average Emotion Loss: 1.5845\n",
            "Epoch [30/30], Batch [5/35], Emotion Loss: 1.6200\n",
            "Epoch [30/30], Batch [10/35], Emotion Loss: 1.6178\n",
            "Epoch [30/30], Batch [15/35], Emotion Loss: 1.4733\n",
            "Epoch [30/30], Batch [20/35], Emotion Loss: 1.6601\n",
            "Epoch [30/30], Batch [25/35], Emotion Loss: 1.5210\n",
            "Epoch [30/30], Batch [30/35], Emotion Loss: 1.6109\n",
            "Epoch [30/30], Batch [35/35], Emotion Loss: 1.6741\n",
            "Epoch [30/30], Average Emotion Loss: 1.5855\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"Cleared CUDA cache\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "data_dir = \"/project/msoleyma_1026/ecp/data/train.json\"\n",
        "embeddings_dir = \"/project/msoleyma_1026/ecp/data/video/train/\"\n",
        "num_emotions = 7  # Number of emotions for classification\n",
        "input_dim = 768  # Dimensionality of your embeddings\n",
        "hidden_dim = 512  # Hidden dimension for the Transformer\n",
        "n_layers = 4  # Number of layers in the Transformer\n",
        "dropout = 0.2  # Dropout probability\n",
        "batch_size = 32\n",
        "num_epochs = 30\n",
        "learning_rate = 2e-5\n",
        "print_interval = 5\n",
        "\n",
        "dataset = ConversationDataset(data_dir, embeddings_dir)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "model = EmotionDetector(input_dim, hidden_dim, num_emotions, n_layers=n_layers, dropout=dropout).to(device)\n",
        "\n",
        "criterion_emotion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss_emotion = 0.0\n",
        "    for batch_idx, (context_embeddings, emotions, convID, utteranceList, labelsList) in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        context_embeddings, emotions = context_embeddings.to(device), emotions.to(device)\n",
        "        prediction_emotion_logits = model(context_embeddings)\n",
        "\n",
        "        outputs_reshaped = prediction_emotion_logits.view(-1, num_emotions)\n",
        "        emotions_reshaped = emotions.view(-1)\n",
        "\n",
        "        # Compute the loss only on non-padded data points\n",
        "        active_outputs = outputs_reshaped[emotions_reshaped != -1]\n",
        "        active_emotions = emotions_reshaped[emotions_reshaped != -1]\n",
        "        loss_emotion = criterion_emotion(active_outputs, active_emotions)\n",
        "\n",
        "        loss_emotion.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss_emotion += loss_emotion.item()\n",
        "\n",
        "        if (batch_idx + 1) % print_interval == 0:\n",
        "            print(f'Epoch [{epoch + 1}/{num_epochs}], Batch [{batch_idx + 1}/{len(dataloader)}], '\n",
        "                  f'Emotion Loss: {loss_emotion.item():.4f}')\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}], Average Emotion Loss: {epoch_loss_emotion / len(dataloader):.4f}')\n",
        "\n",
        "torch.save(model.state_dict(), '/project/msoleyma_1026/ecp/models/emotion/late/emotion_detection_model_late_video.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "L6-ogDmuZ6Yz"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, dataloader, criterion, device, num_emotions):\n",
        "    \"\"\"\n",
        "    Evaluate the model on the given dataloader and compute loss and accuracy, accounting for padding.\n",
        "\n",
        "    Args:\n",
        "    - model (torch.nn.Module): The model to evaluate.\n",
        "    - dataloader (DataLoader): The DataLoader providing the dataset.\n",
        "    - criterion (loss function): The loss function used to evaluate the model's performance.\n",
        "    - device (torch.device): The device computations will be performed on.\n",
        "    - num_emotions (int): Number of emotion categories used in the model output.\n",
        "\n",
        "    Returns:\n",
        "    - float: Average loss over the dataset.\n",
        "    - float: Accuracy, excluding padded data points.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_valid = 0  # Total non-padded data points\n",
        "    all_predictions = []\n",
        "    all_true_emotions = []\n",
        "    predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            context_embeddings, emotions, conversationList, utteranceList, labelsList = batch\n",
        "            context_embeddings, emotions = context_embeddings.to(device), emotions.to(device)\n",
        "\n",
        "            outputs = model(context_embeddings)\n",
        "\n",
        "            # Reshape for loss calculation\n",
        "            outputs_reshaped = outputs.view(-1, num_emotions)\n",
        "            emotions_reshaped = emotions.view(-1)\n",
        "\n",
        "            # Compute the loss only on non-padded data points\n",
        "            active_outputs = outputs_reshaped[emotions_reshaped != -1]\n",
        "            active_emotions = emotions_reshaped[emotions_reshaped != -1]\n",
        "            loss = criterion(active_outputs, active_emotions)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            _, predicted = torch.max(outputs, dim=2)\n",
        "            valid_data = (emotions != -1)\n",
        "            correct_predictions = predicted.eq(emotions) & valid_data\n",
        "            total_correct += correct_predictions.sum().item()\n",
        "            total_valid += valid_data.sum().item()\n",
        "\n",
        "            all_predictions.extend(predicted[valid_data].cpu().numpy())\n",
        "            all_true_emotions.extend(active_emotions.cpu().numpy())\n",
        "\n",
        "            for i, conv_id in enumerate(conversationList):\n",
        "                conv_data = {'conversation_id': int(conv_id), 'utterances': []}\n",
        "                for j, utterance_tensor in enumerate(utteranceList):\n",
        "                    if int(utterance_tensor[i]) != 0:\n",
        "                        conv_data['utterances'].append({'utt_id': int(utterance_tensor[i]), 'emotion': int(predicted[i][j])})\n",
        "                predictions.append(conv_data)\n",
        "\n",
        "    file_path = \"/project/msoleyma_1026/ecp/data/predictions/late_fusion_predictions_video.json\"\n",
        "\n",
        "    with open(file_path, 'w') as file:\n",
        "        json.dump(predictions, file, indent=4)\n",
        "    \n",
        "    f1_score_val = f1_score(all_true_emotions, all_predictions, average='weighted')\n",
        "    classification_report_result = classification_report(all_true_emotions, all_predictions)\n",
        "\n",
        "    average_loss = total_loss / len(dataloader)\n",
        "    accuracy = total_correct / total_valid if total_valid > 0 else 0\n",
        "    \n",
        "    return average_loss, accuracy, f1_score_val, classification_report_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "yY6xGfBD87CI",
        "outputId": "09e2bf45-44bc-43af-fae5-e42fc9cf3718"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss: 1.5818, Validation Accuracy: 0.4369%\n",
            "F1 Score: 26.57%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       333\n",
            "           1       0.00      0.00      0.00        79\n",
            "           2       0.00      0.00      0.00        56\n",
            "           3       0.00      0.00      0.00       429\n",
            "           4       0.44      1.00      0.61      1121\n",
            "           5       0.00      0.00      0.00       241\n",
            "           6       0.00      0.00      0.00       307\n",
            "\n",
            "    accuracy                           0.44      2566\n",
            "   macro avg       0.06      0.14      0.09      2566\n",
            "weighted avg       0.19      0.44      0.27      2566\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/project/msoleyma_1026/ecp/ecp-venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/project/msoleyma_1026/ecp/ecp-venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/project/msoleyma_1026/ecp/ecp-venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "data_dir_test = \"/project/msoleyma_1026/ecp/data/test.json\"\n",
        "embeddings_dir_test = \"/project/msoleyma_1026/ecp/data/video/test/\"\n",
        "\n",
        "dataset = ConversationDataset(data_dir_test, embeddings_dir_test)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "validation_dataloader = DataLoader(dataset, batch_size=8, shuffle=False)  # Same dataloader for simplicity\n",
        "model.to(device)\n",
        "criterion_emotion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Call the evaluation function\n",
        "val_loss, val_accuracy, val_f1, val_class_report = evaluate_model(model, validation_dataloader, criterion_emotion, device, num_emotions=7)\n",
        "print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}%\")\n",
        "print(f\"F1 Score: {val_f1 * 100:.2f}%\")\n",
        "print(val_class_report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cX-VZF5P87CJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
