{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "sQgNYU5ub1qg",
        "outputId": "a6291b16-9e16-4581-a80b-85601b076057"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, 1024)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "x = np.load(\"/project/msoleyma_1026/ecp/data/audio/train-emotion/dia1utt1.npy\")\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "WWi29yesQ_VI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "import torch.nn.functional as F\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "from sklearn.utils.class_weight import compute_class_weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-ebSo_3zAfqz"
      },
      "outputs": [],
      "source": [
        "class ConversationDataset(Dataset):\n",
        "    def __init__(self, data_dir, embeddings_dir):\n",
        "        self.data_dir = data_dir\n",
        "        self.embeddings_dir = embeddings_dir\n",
        "        self.conversations = self.load_conversations()\n",
        "        self.emotion_list = sorted(['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness','surprise'])\n",
        "        self.encoder = LabelEncoder()\n",
        "        self.encoder.fit(self.emotion_list)\n",
        "        self.class_weights = self.compute_class_weights()\n",
        "\n",
        "    def compute_class_weights(self):\n",
        "        all_labels = []\n",
        "        for conversation in self.conversations:\n",
        "            for utterance in conversation['conversation']:\n",
        "                all_labels.append(utterance['emotion'])\n",
        "        class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(all_labels), y=all_labels)\n",
        "        return torch.tensor(class_weights, dtype=torch.float32)\n",
        "\n",
        "    def encode_emotion(self, emotion):\n",
        "        encoded = self.encoder.transform([emotion])[0]\n",
        "        return encoded\n",
        "\n",
        "    def load_conversations(self):\n",
        "        with open(self.data_dir, 'r') as file:\n",
        "            conversations = json.load(file)\n",
        "        # print(f\"Loaded {len(conversations)} conversations\")\n",
        "        return conversations\n",
        "\n",
        "    def load_embeddings(self, video_name):\n",
        "        video_name = video_name.split('.')[0]\n",
        "\n",
        "        embedding_file = os.path.join(self.embeddings_dir, f'{video_name}.npy')\n",
        "        embedding = np.load(embedding_file)\n",
        "        return embedding\n",
        "\n",
        "    def positional_encoding(self, embeddings):\n",
        "        seq_length = embeddings.shape[0]\n",
        "        embedding_dim = embeddings.shape[1]\n",
        "        position_enc = torch.zeros(seq_length, embedding_dim)\n",
        "        position = torch.arange(0, seq_length, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-np.log(10000.0) / embedding_dim))\n",
        "        position_enc[:, 0::2] = torch.sin(position * div_term)\n",
        "        position_enc[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        return position_enc\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.conversations)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        conversation = self.conversations[idx]\n",
        "        context_embeddings = []\n",
        "        emotions = []\n",
        "        conversation_ID = conversation['conversation_ID']\n",
        "        utterance_list = []\n",
        "        labels_list = []\n",
        "\n",
        "        for utterance in conversation['conversation']:\n",
        "            video_name = utterance['video_name']\n",
        "            utterance_ID = utterance['utterance_ID']\n",
        "            utterance_list.append(utterance_ID)\n",
        "            emotion = self.encode_emotion(utterance['emotion'])\n",
        "            labels_list.append(emotion)\n",
        "            embedding = self.load_embeddings(video_name)\n",
        "            context_embeddings.append(torch.from_numpy(embedding))\n",
        "            emotions.append(emotion)\n",
        "\n",
        "        max_seq_length = 33\n",
        "        utterance_list = utterance_list + [0] * (33 - len(utterance_list)) if len(utterance_list) < 33 else utterance_list\n",
        "        labels_list = labels_list + [-1] * (33 - len(labels_list)) if len(labels_list) < 33 else labels_list\n",
        "\n",
        "        padded_embeddings = []\n",
        "        num_to_add = 0\n",
        "        if len(context_embeddings) < max_seq_length:\n",
        "            num_to_add = max_seq_length - len(context_embeddings)\n",
        "            zero_tensor = torch.zeros((1,1024), dtype=torch.float32)\n",
        "            context_embeddings += [zero_tensor] * num_to_add\n",
        "\n",
        "        context_embeddings_padded = torch.cat(context_embeddings, dim=0)\n",
        "\n",
        "        positional_encodings = self.positional_encoding(context_embeddings_padded)\n",
        "        context_embeddings_with_pos = context_embeddings_padded + positional_encodings\n",
        "\n",
        "\n",
        "        emotions += [-1] * num_to_add\n",
        "        encoded_emotions_tensor = torch.tensor(emotions, dtype=torch.long)\n",
        "\n",
        "\n",
        "        return context_embeddings_with_pos, encoded_emotions_tensor, torch.tensor(conversation_ID), utterance_list, labels_list\n",
        "\n",
        "# class EmotionDetector(nn.Module):\n",
        "#     def __init__(self, input_dim, hidden_dim, num_emotions, n_layers=2, dropout=0.2):\n",
        "#         super(EmotionDetector, self).__init__()\n",
        "#         self.input_dim = input_dim\n",
        "#         self.hidden_dim = hidden_dim\n",
        "#         self.num_emotions = num_emotions\n",
        "#         self.n_layers = n_layers\n",
        "\n",
        "#         encoder_layers = TransformerEncoderLayer(d_model=input_dim, nhead=4, dim_feedforward=hidden_dim, dropout=dropout, batch_first=True)\n",
        "#         self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers=n_layers)\n",
        "\n",
        "#         self.decoder_emotion = nn.Linear(input_dim, num_emotions)\n",
        "#         print(\"Initialized EmotionDetector\")\n",
        "\n",
        "#     def forward(self, context_embeddings):\n",
        "#         encoded_context = self.transformer_encoder(context_embeddings)\n",
        "#         prediction_emotion = self.decoder_emotion(encoded_context)  # (batch_size, seq_length, num_emotions)\n",
        "#         return prediction_emotion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EmotionDetector(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_emotions, n_layers=4, dropout=0.2, class_weights=None):\n",
        "        super(EmotionDetector, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_emotions = num_emotions\n",
        "        self.n_layers = n_layers\n",
        "        self.class_weights = class_weights\n",
        "\n",
        "        # Forward Transformer encoder\n",
        "        self.forward_encoder = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model=input_dim, nhead=4, dim_feedforward=hidden_dim, dropout=dropout),\n",
        "            num_layers=n_layers)\n",
        "\n",
        "        # Backward Transformer encoder\n",
        "        self.backward_encoder = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model=input_dim, nhead=4, dim_feedforward=hidden_dim, dropout=dropout),\n",
        "            num_layers=n_layers)\n",
        "\n",
        "        # Linear layer for emotion prediction\n",
        "        self.decoder_emotion = nn.Sequential(\n",
        "            nn.Linear(input_dim * 2, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, num_emotions))\n",
        "\n",
        "        print(\"Initialized EmotionDetector\")\n",
        "\n",
        "    def forward(self, context_embeddings):\n",
        "        # Forward pass\n",
        "        encoded_forward = self.forward_encoder(context_embeddings)\n",
        "\n",
        "        # Reverse the sequence for backward pass\n",
        "        context_embeddings_reversed = torch.flip(context_embeddings, dims=[1])\n",
        "\n",
        "        # Backward pass\n",
        "        encoded_backward = self.backward_encoder(context_embeddings_reversed)\n",
        "\n",
        "        # Concatenate forward and backward outputs\n",
        "        encoded_context = torch.cat((encoded_forward, encoded_backward), dim=-1)\n",
        "\n",
        "        # Predict emotions\n",
        "        prediction_emotion = self.decoder_emotion(encoded_context)\n",
        "\n",
        "        return prediction_emotion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_e9CkeiQfekH",
        "outputId": "f10fe488-a2da-4d2d-fe76-44e4ebba7c5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleared CUDA cache\n",
            "Using device: cuda\n",
            "tensor([1.2317, 4.7134, 4.9811, 0.8435, 0.3284, 1.7428, 1.0300],\n",
            "       device='cuda:0')\n",
            "Initialized EmotionDetector\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/project/msoleyma_1026/ecp/ecp-venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/50], Batch [5/35], Emotion Loss: 1.9595\n",
            "Epoch [1/50], Batch [10/35], Emotion Loss: 1.9316\n",
            "Epoch [1/50], Batch [15/35], Emotion Loss: 1.9466\n",
            "Epoch [1/50], Batch [20/35], Emotion Loss: 1.9429\n",
            "Epoch [1/50], Batch [25/35], Emotion Loss: 1.9459\n",
            "Epoch [1/50], Batch [30/35], Emotion Loss: 1.9530\n",
            "Epoch [1/50], Batch [35/35], Emotion Loss: 1.9335\n",
            "Epoch [1/50], Average Emotion Loss: 1.9482\n",
            "Epoch [2/50], Batch [5/35], Emotion Loss: 1.9169\n",
            "Epoch [2/50], Batch [10/35], Emotion Loss: 1.9625\n",
            "Epoch [2/50], Batch [15/35], Emotion Loss: 1.9431\n",
            "Epoch [2/50], Batch [20/35], Emotion Loss: 1.9689\n",
            "Epoch [2/50], Batch [25/35], Emotion Loss: 1.9407\n",
            "Epoch [2/50], Batch [30/35], Emotion Loss: 1.9301\n",
            "Epoch [2/50], Batch [35/35], Emotion Loss: 1.9199\n",
            "Epoch [2/50], Average Emotion Loss: 1.9426\n",
            "Epoch [3/50], Batch [5/35], Emotion Loss: 1.9527\n",
            "Epoch [3/50], Batch [10/35], Emotion Loss: 1.9617\n",
            "Epoch [3/50], Batch [15/35], Emotion Loss: 1.9543\n",
            "Epoch [3/50], Batch [20/35], Emotion Loss: 1.9342\n",
            "Epoch [3/50], Batch [25/35], Emotion Loss: 1.9270\n",
            "Epoch [3/50], Batch [30/35], Emotion Loss: 1.9220\n",
            "Epoch [3/50], Batch [35/35], Emotion Loss: 1.9263\n",
            "Epoch [3/50], Average Emotion Loss: 1.9419\n",
            "Epoch [4/50], Batch [5/35], Emotion Loss: 1.9429\n",
            "Epoch [4/50], Batch [10/35], Emotion Loss: 1.9332\n",
            "Epoch [4/50], Batch [15/35], Emotion Loss: 1.9355\n",
            "Epoch [4/50], Batch [20/35], Emotion Loss: 1.9334\n",
            "Epoch [4/50], Batch [25/35], Emotion Loss: 1.9433\n",
            "Epoch [4/50], Batch [30/35], Emotion Loss: 1.9465\n",
            "Epoch [4/50], Batch [35/35], Emotion Loss: 1.9593\n",
            "Epoch [4/50], Average Emotion Loss: 1.9399\n",
            "Epoch [5/50], Batch [5/35], Emotion Loss: 1.9210\n",
            "Epoch [5/50], Batch [10/35], Emotion Loss: 1.9355\n",
            "Epoch [5/50], Batch [15/35], Emotion Loss: 1.9499\n",
            "Epoch [5/50], Batch [20/35], Emotion Loss: 1.9440\n",
            "Epoch [5/50], Batch [25/35], Emotion Loss: 1.9425\n",
            "Epoch [5/50], Batch [30/35], Emotion Loss: 1.9266\n",
            "Epoch [5/50], Batch [35/35], Emotion Loss: 1.9714\n",
            "Epoch [5/50], Average Emotion Loss: 1.9396\n",
            "Epoch [6/50], Batch [5/35], Emotion Loss: 1.9415\n",
            "Epoch [6/50], Batch [10/35], Emotion Loss: 1.9451\n",
            "Epoch [6/50], Batch [15/35], Emotion Loss: 1.9353\n",
            "Epoch [6/50], Batch [20/35], Emotion Loss: 1.9384\n",
            "Epoch [6/50], Batch [25/35], Emotion Loss: 1.9338\n",
            "Epoch [6/50], Batch [30/35], Emotion Loss: 1.9302\n",
            "Epoch [6/50], Batch [35/35], Emotion Loss: 1.9290\n",
            "Epoch [6/50], Average Emotion Loss: 1.9394\n",
            "Epoch [7/50], Batch [5/35], Emotion Loss: 1.9544\n",
            "Epoch [7/50], Batch [10/35], Emotion Loss: 1.9386\n",
            "Epoch [7/50], Batch [15/35], Emotion Loss: 1.9479\n",
            "Epoch [7/50], Batch [20/35], Emotion Loss: 1.9349\n",
            "Epoch [7/50], Batch [25/35], Emotion Loss: 1.9314\n",
            "Epoch [7/50], Batch [30/35], Emotion Loss: 1.9320\n",
            "Epoch [7/50], Batch [35/35], Emotion Loss: 1.9629\n",
            "Epoch [7/50], Average Emotion Loss: 1.9373\n",
            "Epoch [8/50], Batch [5/35], Emotion Loss: 1.9372\n",
            "Epoch [8/50], Batch [10/35], Emotion Loss: 1.9538\n",
            "Epoch [8/50], Batch [15/35], Emotion Loss: 1.9532\n",
            "Epoch [8/50], Batch [20/35], Emotion Loss: 1.9202\n",
            "Epoch [8/50], Batch [25/35], Emotion Loss: 1.9375\n",
            "Epoch [8/50], Batch [30/35], Emotion Loss: 1.9427\n",
            "Epoch [8/50], Batch [35/35], Emotion Loss: 1.9656\n",
            "Epoch [8/50], Average Emotion Loss: 1.9379\n",
            "Epoch [9/50], Batch [5/35], Emotion Loss: 1.9271\n",
            "Epoch [9/50], Batch [10/35], Emotion Loss: 1.9296\n",
            "Epoch [9/50], Batch [15/35], Emotion Loss: 1.9282\n",
            "Epoch [9/50], Batch [20/35], Emotion Loss: 1.9591\n",
            "Epoch [9/50], Batch [25/35], Emotion Loss: 1.9334\n",
            "Epoch [9/50], Batch [30/35], Emotion Loss: 1.9489\n",
            "Epoch [9/50], Batch [35/35], Emotion Loss: 1.9497\n",
            "Epoch [9/50], Average Emotion Loss: 1.9369\n",
            "Epoch [10/50], Batch [5/35], Emotion Loss: 1.9196\n",
            "Epoch [10/50], Batch [10/35], Emotion Loss: 1.9437\n",
            "Epoch [10/50], Batch [15/35], Emotion Loss: 1.9268\n",
            "Epoch [10/50], Batch [20/35], Emotion Loss: 1.9559\n",
            "Epoch [10/50], Batch [25/35], Emotion Loss: 1.9337\n",
            "Epoch [10/50], Batch [30/35], Emotion Loss: 1.9443\n",
            "Epoch [10/50], Batch [35/35], Emotion Loss: 1.9267\n",
            "Epoch [10/50], Average Emotion Loss: 1.9359\n",
            "Epoch [11/50], Batch [5/35], Emotion Loss: 1.9154\n",
            "Epoch [11/50], Batch [10/35], Emotion Loss: 1.9473\n",
            "Epoch [11/50], Batch [15/35], Emotion Loss: 1.9577\n",
            "Epoch [11/50], Batch [20/35], Emotion Loss: 1.9540\n",
            "Epoch [11/50], Batch [25/35], Emotion Loss: 1.9563\n",
            "Epoch [11/50], Batch [30/35], Emotion Loss: 1.9135\n",
            "Epoch [11/50], Batch [35/35], Emotion Loss: 1.9572\n",
            "Epoch [11/50], Average Emotion Loss: 1.9345\n",
            "Epoch [12/50], Batch [5/35], Emotion Loss: 1.9276\n",
            "Epoch [12/50], Batch [10/35], Emotion Loss: 1.9224\n",
            "Epoch [12/50], Batch [15/35], Emotion Loss: 1.9338\n",
            "Epoch [12/50], Batch [20/35], Emotion Loss: 1.9211\n",
            "Epoch [12/50], Batch [25/35], Emotion Loss: 1.9338\n",
            "Epoch [12/50], Batch [30/35], Emotion Loss: 1.9569\n",
            "Epoch [12/50], Batch [35/35], Emotion Loss: 1.9377\n",
            "Epoch [12/50], Average Emotion Loss: 1.9329\n",
            "Epoch [13/50], Batch [5/35], Emotion Loss: 1.9165\n",
            "Epoch [13/50], Batch [10/35], Emotion Loss: 1.9296\n",
            "Epoch [13/50], Batch [15/35], Emotion Loss: 1.9389\n",
            "Epoch [13/50], Batch [20/35], Emotion Loss: 1.9659\n",
            "Epoch [13/50], Batch [25/35], Emotion Loss: 1.9567\n",
            "Epoch [13/50], Batch [30/35], Emotion Loss: 1.9319\n",
            "Epoch [13/50], Batch [35/35], Emotion Loss: 1.9430\n",
            "Epoch [13/50], Average Emotion Loss: 1.9314\n",
            "Epoch [14/50], Batch [5/35], Emotion Loss: 1.9358\n",
            "Epoch [14/50], Batch [10/35], Emotion Loss: 1.9290\n",
            "Epoch [14/50], Batch [15/35], Emotion Loss: 1.9374\n",
            "Epoch [14/50], Batch [20/35], Emotion Loss: 1.9250\n",
            "Epoch [14/50], Batch [25/35], Emotion Loss: 1.9476\n",
            "Epoch [14/50], Batch [30/35], Emotion Loss: 1.9285\n",
            "Epoch [14/50], Batch [35/35], Emotion Loss: 1.9591\n",
            "Epoch [14/50], Average Emotion Loss: 1.9328\n",
            "Epoch [15/50], Batch [5/35], Emotion Loss: 1.9327\n",
            "Epoch [15/50], Batch [10/35], Emotion Loss: 1.9177\n",
            "Epoch [15/50], Batch [15/35], Emotion Loss: 1.9508\n",
            "Epoch [15/50], Batch [20/35], Emotion Loss: 1.9249\n",
            "Epoch [15/50], Batch [25/35], Emotion Loss: 1.9410\n",
            "Epoch [15/50], Batch [30/35], Emotion Loss: 1.9473\n",
            "Epoch [15/50], Batch [35/35], Emotion Loss: 1.9388\n",
            "Epoch [15/50], Average Emotion Loss: 1.9291\n",
            "Epoch [16/50], Batch [5/35], Emotion Loss: 1.9283\n",
            "Epoch [16/50], Batch [10/35], Emotion Loss: 1.9254\n",
            "Epoch [16/50], Batch [15/35], Emotion Loss: 1.9403\n",
            "Epoch [16/50], Batch [20/35], Emotion Loss: 1.9193\n",
            "Epoch [16/50], Batch [25/35], Emotion Loss: 1.9086\n",
            "Epoch [16/50], Batch [30/35], Emotion Loss: 1.9135\n",
            "Epoch [16/50], Batch [35/35], Emotion Loss: 1.9249\n",
            "Epoch [16/50], Average Emotion Loss: 1.9270\n",
            "Epoch [17/50], Batch [5/35], Emotion Loss: 1.9132\n",
            "Epoch [17/50], Batch [10/35], Emotion Loss: 1.9184\n",
            "Epoch [17/50], Batch [15/35], Emotion Loss: 1.9394\n",
            "Epoch [17/50], Batch [20/35], Emotion Loss: 1.9593\n",
            "Epoch [17/50], Batch [25/35], Emotion Loss: 1.8835\n",
            "Epoch [17/50], Batch [30/35], Emotion Loss: 1.9168\n",
            "Epoch [17/50], Batch [35/35], Emotion Loss: 1.9292\n",
            "Epoch [17/50], Average Emotion Loss: 1.9230\n",
            "Epoch [18/50], Batch [5/35], Emotion Loss: 1.8958\n",
            "Epoch [18/50], Batch [10/35], Emotion Loss: 1.8795\n",
            "Epoch [18/50], Batch [15/35], Emotion Loss: 1.9464\n",
            "Epoch [18/50], Batch [20/35], Emotion Loss: 1.9238\n",
            "Epoch [18/50], Batch [25/35], Emotion Loss: 1.9196\n",
            "Epoch [18/50], Batch [30/35], Emotion Loss: 1.9022\n",
            "Epoch [18/50], Batch [35/35], Emotion Loss: 1.9349\n",
            "Epoch [18/50], Average Emotion Loss: 1.9202\n",
            "Epoch [19/50], Batch [5/35], Emotion Loss: 1.9225\n",
            "Epoch [19/50], Batch [10/35], Emotion Loss: 1.9013\n",
            "Epoch [19/50], Batch [15/35], Emotion Loss: 1.9073\n",
            "Epoch [19/50], Batch [20/35], Emotion Loss: 1.9504\n",
            "Epoch [19/50], Batch [25/35], Emotion Loss: 1.8848\n",
            "Epoch [19/50], Batch [30/35], Emotion Loss: 1.9219\n",
            "Epoch [19/50], Batch [35/35], Emotion Loss: 1.9316\n",
            "Epoch [19/50], Average Emotion Loss: 1.9156\n",
            "Epoch [20/50], Batch [5/35], Emotion Loss: 1.9227\n",
            "Epoch [20/50], Batch [10/35], Emotion Loss: 1.8795\n",
            "Epoch [20/50], Batch [15/35], Emotion Loss: 1.8751\n",
            "Epoch [20/50], Batch [20/35], Emotion Loss: 1.8597\n",
            "Epoch [20/50], Batch [25/35], Emotion Loss: 1.8889\n",
            "Epoch [20/50], Batch [30/35], Emotion Loss: 1.9357\n",
            "Epoch [20/50], Batch [35/35], Emotion Loss: 1.9325\n",
            "Epoch [20/50], Average Emotion Loss: 1.9129\n",
            "Epoch [21/50], Batch [5/35], Emotion Loss: 1.8774\n",
            "Epoch [21/50], Batch [10/35], Emotion Loss: 1.9052\n",
            "Epoch [21/50], Batch [15/35], Emotion Loss: 1.9108\n",
            "Epoch [21/50], Batch [20/35], Emotion Loss: 1.9433\n",
            "Epoch [21/50], Batch [25/35], Emotion Loss: 1.9326\n",
            "Epoch [21/50], Batch [30/35], Emotion Loss: 1.9120\n",
            "Epoch [21/50], Batch [35/35], Emotion Loss: 1.9192\n",
            "Epoch [21/50], Average Emotion Loss: 1.9129\n",
            "Epoch [22/50], Batch [5/35], Emotion Loss: 1.9212\n",
            "Epoch [22/50], Batch [10/35], Emotion Loss: 1.8682\n",
            "Epoch [22/50], Batch [15/35], Emotion Loss: 1.9129\n",
            "Epoch [22/50], Batch [20/35], Emotion Loss: 1.9418\n",
            "Epoch [22/50], Batch [25/35], Emotion Loss: 1.9228\n",
            "Epoch [22/50], Batch [30/35], Emotion Loss: 1.9213\n",
            "Epoch [22/50], Batch [35/35], Emotion Loss: 1.8604\n",
            "Epoch [22/50], Average Emotion Loss: 1.9104\n",
            "Epoch [23/50], Batch [5/35], Emotion Loss: 1.9150\n",
            "Epoch [23/50], Batch [10/35], Emotion Loss: 1.9202\n",
            "Epoch [23/50], Batch [15/35], Emotion Loss: 1.9229\n",
            "Epoch [23/50], Batch [20/35], Emotion Loss: 1.9045\n",
            "Epoch [23/50], Batch [25/35], Emotion Loss: 1.9056\n",
            "Epoch [23/50], Batch [30/35], Emotion Loss: 1.8889\n",
            "Epoch [23/50], Batch [35/35], Emotion Loss: 1.9304\n",
            "Epoch [23/50], Average Emotion Loss: 1.9143\n",
            "Epoch [24/50], Batch [5/35], Emotion Loss: 1.8952\n",
            "Epoch [24/50], Batch [10/35], Emotion Loss: 1.9393\n",
            "Epoch [24/50], Batch [15/35], Emotion Loss: 1.9328\n",
            "Epoch [24/50], Batch [20/35], Emotion Loss: 1.9228\n",
            "Epoch [24/50], Batch [25/35], Emotion Loss: 1.8860\n",
            "Epoch [24/50], Batch [30/35], Emotion Loss: 1.9325\n",
            "Epoch [24/50], Batch [35/35], Emotion Loss: 1.8772\n",
            "Epoch [24/50], Average Emotion Loss: 1.9037\n",
            "Epoch [25/50], Batch [5/35], Emotion Loss: 1.8789\n",
            "Epoch [25/50], Batch [10/35], Emotion Loss: 1.9081\n",
            "Epoch [25/50], Batch [15/35], Emotion Loss: 1.8826\n",
            "Epoch [25/50], Batch [20/35], Emotion Loss: 1.8859\n",
            "Epoch [25/50], Batch [25/35], Emotion Loss: 1.9190\n",
            "Epoch [25/50], Batch [30/35], Emotion Loss: 1.8737\n",
            "Epoch [25/50], Batch [35/35], Emotion Loss: 1.9221\n",
            "Epoch [25/50], Average Emotion Loss: 1.9049\n",
            "Epoch [26/50], Batch [5/35], Emotion Loss: 1.9000\n",
            "Epoch [26/50], Batch [10/35], Emotion Loss: 1.9018\n",
            "Epoch [26/50], Batch [15/35], Emotion Loss: 1.8874\n",
            "Epoch [26/50], Batch [20/35], Emotion Loss: 1.9133\n",
            "Epoch [26/50], Batch [25/35], Emotion Loss: 1.9002\n",
            "Epoch [26/50], Batch [30/35], Emotion Loss: 1.9076\n",
            "Epoch [26/50], Batch [35/35], Emotion Loss: 1.9348\n",
            "Epoch [26/50], Average Emotion Loss: 1.9027\n",
            "Epoch [27/50], Batch [5/35], Emotion Loss: 1.9086\n",
            "Epoch [27/50], Batch [10/35], Emotion Loss: 1.8620\n",
            "Epoch [27/50], Batch [15/35], Emotion Loss: 1.9266\n",
            "Epoch [27/50], Batch [20/35], Emotion Loss: 1.9215\n",
            "Epoch [27/50], Batch [25/35], Emotion Loss: 1.9052\n",
            "Epoch [27/50], Batch [30/35], Emotion Loss: 1.8661\n",
            "Epoch [27/50], Batch [35/35], Emotion Loss: 1.9019\n",
            "Epoch [27/50], Average Emotion Loss: 1.8979\n",
            "Epoch [28/50], Batch [5/35], Emotion Loss: 1.9074\n",
            "Epoch [28/50], Batch [10/35], Emotion Loss: 1.9278\n",
            "Epoch [28/50], Batch [15/35], Emotion Loss: 1.9017\n",
            "Epoch [28/50], Batch [20/35], Emotion Loss: 1.8787\n",
            "Epoch [28/50], Batch [25/35], Emotion Loss: 1.9165\n",
            "Epoch [28/50], Batch [30/35], Emotion Loss: 1.8674\n",
            "Epoch [28/50], Batch [35/35], Emotion Loss: 1.8695\n",
            "Epoch [28/50], Average Emotion Loss: 1.8973\n",
            "Epoch [29/50], Batch [5/35], Emotion Loss: 1.9049\n",
            "Epoch [29/50], Batch [10/35], Emotion Loss: 1.8535\n",
            "Epoch [29/50], Batch [15/35], Emotion Loss: 1.9300\n",
            "Epoch [29/50], Batch [20/35], Emotion Loss: 1.8878\n",
            "Epoch [29/50], Batch [25/35], Emotion Loss: 1.9183\n",
            "Epoch [29/50], Batch [30/35], Emotion Loss: 1.8861\n",
            "Epoch [29/50], Batch [35/35], Emotion Loss: 1.8835\n",
            "Epoch [29/50], Average Emotion Loss: 1.8958\n",
            "Epoch [30/50], Batch [5/35], Emotion Loss: 1.9091\n",
            "Epoch [30/50], Batch [10/35], Emotion Loss: 1.8474\n",
            "Epoch [30/50], Batch [15/35], Emotion Loss: 1.8917\n",
            "Epoch [30/50], Batch [20/35], Emotion Loss: 1.8884\n",
            "Epoch [30/50], Batch [25/35], Emotion Loss: 1.8984\n",
            "Epoch [30/50], Batch [30/35], Emotion Loss: 1.9063\n",
            "Epoch [30/50], Batch [35/35], Emotion Loss: 1.8819\n",
            "Epoch [30/50], Average Emotion Loss: 1.8922\n",
            "Epoch [31/50], Batch [5/35], Emotion Loss: 1.8388\n",
            "Epoch [31/50], Batch [10/35], Emotion Loss: 1.8793\n",
            "Epoch [31/50], Batch [15/35], Emotion Loss: 1.8998\n",
            "Epoch [31/50], Batch [20/35], Emotion Loss: 1.9085\n",
            "Epoch [31/50], Batch [25/35], Emotion Loss: 1.8891\n",
            "Epoch [31/50], Batch [30/35], Emotion Loss: 1.8787\n",
            "Epoch [31/50], Batch [35/35], Emotion Loss: 1.9521\n",
            "Epoch [31/50], Average Emotion Loss: 1.8925\n",
            "Epoch [32/50], Batch [5/35], Emotion Loss: 1.8521\n",
            "Epoch [32/50], Batch [10/35], Emotion Loss: 1.9064\n",
            "Epoch [32/50], Batch [15/35], Emotion Loss: 1.8875\n",
            "Epoch [32/50], Batch [20/35], Emotion Loss: 1.8870\n",
            "Epoch [32/50], Batch [25/35], Emotion Loss: 1.8663\n",
            "Epoch [32/50], Batch [30/35], Emotion Loss: 1.9255\n",
            "Epoch [32/50], Batch [35/35], Emotion Loss: 1.9381\n",
            "Epoch [32/50], Average Emotion Loss: 1.8883\n",
            "Epoch [33/50], Batch [5/35], Emotion Loss: 1.9003\n",
            "Epoch [33/50], Batch [10/35], Emotion Loss: 1.8870\n",
            "Epoch [33/50], Batch [15/35], Emotion Loss: 1.8787\n",
            "Epoch [33/50], Batch [20/35], Emotion Loss: 1.8529\n",
            "Epoch [33/50], Batch [25/35], Emotion Loss: 1.9200\n",
            "Epoch [33/50], Batch [30/35], Emotion Loss: 1.8419\n",
            "Epoch [33/50], Batch [35/35], Emotion Loss: 1.9036\n",
            "Epoch [33/50], Average Emotion Loss: 1.8876\n",
            "Epoch [34/50], Batch [5/35], Emotion Loss: 1.8964\n",
            "Epoch [34/50], Batch [10/35], Emotion Loss: 1.8960\n",
            "Epoch [34/50], Batch [15/35], Emotion Loss: 1.8949\n",
            "Epoch [34/50], Batch [20/35], Emotion Loss: 1.9091\n",
            "Epoch [34/50], Batch [25/35], Emotion Loss: 1.9106\n",
            "Epoch [34/50], Batch [30/35], Emotion Loss: 1.9066\n",
            "Epoch [34/50], Batch [35/35], Emotion Loss: 1.8738\n",
            "Epoch [34/50], Average Emotion Loss: 1.8849\n",
            "Epoch [35/50], Batch [5/35], Emotion Loss: 1.9106\n",
            "Epoch [35/50], Batch [10/35], Emotion Loss: 1.8672\n",
            "Epoch [35/50], Batch [15/35], Emotion Loss: 1.9232\n",
            "Epoch [35/50], Batch [20/35], Emotion Loss: 1.9032\n",
            "Epoch [35/50], Batch [25/35], Emotion Loss: 1.9183\n",
            "Epoch [35/50], Batch [30/35], Emotion Loss: 1.8519\n",
            "Epoch [35/50], Batch [35/35], Emotion Loss: 1.8794\n",
            "Epoch [35/50], Average Emotion Loss: 1.8860\n",
            "Epoch [36/50], Batch [5/35], Emotion Loss: 1.8716\n",
            "Epoch [36/50], Batch [10/35], Emotion Loss: 1.8781\n",
            "Epoch [36/50], Batch [15/35], Emotion Loss: 1.8768\n",
            "Epoch [36/50], Batch [20/35], Emotion Loss: 1.9448\n",
            "Epoch [36/50], Batch [25/35], Emotion Loss: 1.8922\n",
            "Epoch [36/50], Batch [30/35], Emotion Loss: 1.8936\n",
            "Epoch [36/50], Batch [35/35], Emotion Loss: 1.8768\n",
            "Epoch [36/50], Average Emotion Loss: 1.8819\n",
            "Epoch [37/50], Batch [5/35], Emotion Loss: 1.8903\n",
            "Epoch [37/50], Batch [10/35], Emotion Loss: 1.8697\n",
            "Epoch [37/50], Batch [15/35], Emotion Loss: 1.9168\n",
            "Epoch [37/50], Batch [20/35], Emotion Loss: 1.8883\n",
            "Epoch [37/50], Batch [25/35], Emotion Loss: 1.8716\n",
            "Epoch [37/50], Batch [30/35], Emotion Loss: 1.8686\n",
            "Epoch [37/50], Batch [35/35], Emotion Loss: 1.8449\n",
            "Epoch [37/50], Average Emotion Loss: 1.8814\n",
            "Epoch [38/50], Batch [5/35], Emotion Loss: 1.8169\n",
            "Epoch [38/50], Batch [10/35], Emotion Loss: 1.8717\n",
            "Epoch [38/50], Batch [15/35], Emotion Loss: 1.9015\n",
            "Epoch [38/50], Batch [20/35], Emotion Loss: 1.8573\n",
            "Epoch [38/50], Batch [25/35], Emotion Loss: 1.8730\n",
            "Epoch [38/50], Batch [30/35], Emotion Loss: 1.8585\n",
            "Epoch [38/50], Batch [35/35], Emotion Loss: 1.8641\n",
            "Epoch [38/50], Average Emotion Loss: 1.8793\n",
            "Epoch [39/50], Batch [5/35], Emotion Loss: 1.8644\n",
            "Epoch [39/50], Batch [10/35], Emotion Loss: 1.8788\n",
            "Epoch [39/50], Batch [15/35], Emotion Loss: 1.8456\n",
            "Epoch [39/50], Batch [20/35], Emotion Loss: 1.8297\n",
            "Epoch [39/50], Batch [25/35], Emotion Loss: 1.8524\n",
            "Epoch [39/50], Batch [30/35], Emotion Loss: 1.8448\n",
            "Epoch [39/50], Batch [35/35], Emotion Loss: 1.8067\n",
            "Epoch [39/50], Average Emotion Loss: 1.8760\n",
            "Epoch [40/50], Batch [5/35], Emotion Loss: 1.8566\n",
            "Epoch [40/50], Batch [10/35], Emotion Loss: 1.8005\n",
            "Epoch [40/50], Batch [15/35], Emotion Loss: 1.8812\n",
            "Epoch [40/50], Batch [20/35], Emotion Loss: 1.8524\n",
            "Epoch [40/50], Batch [25/35], Emotion Loss: 1.8926\n",
            "Epoch [40/50], Batch [30/35], Emotion Loss: 1.8281\n",
            "Epoch [40/50], Batch [35/35], Emotion Loss: 1.9027\n",
            "Epoch [40/50], Average Emotion Loss: 1.8730\n",
            "Epoch [41/50], Batch [5/35], Emotion Loss: 1.8426\n",
            "Epoch [41/50], Batch [10/35], Emotion Loss: 1.8520\n",
            "Epoch [41/50], Batch [15/35], Emotion Loss: 1.8659\n",
            "Epoch [41/50], Batch [20/35], Emotion Loss: 1.8599\n",
            "Epoch [41/50], Batch [25/35], Emotion Loss: 1.8781\n",
            "Epoch [41/50], Batch [30/35], Emotion Loss: 1.8607\n",
            "Epoch [41/50], Batch [35/35], Emotion Loss: 1.8295\n",
            "Epoch [41/50], Average Emotion Loss: 1.8700\n",
            "Epoch [42/50], Batch [5/35], Emotion Loss: 1.8848\n",
            "Epoch [42/50], Batch [10/35], Emotion Loss: 1.8335\n",
            "Epoch [42/50], Batch [15/35], Emotion Loss: 1.8390\n",
            "Epoch [42/50], Batch [20/35], Emotion Loss: 1.9394\n",
            "Epoch [42/50], Batch [25/35], Emotion Loss: 1.8294\n",
            "Epoch [42/50], Batch [30/35], Emotion Loss: 1.9014\n",
            "Epoch [42/50], Batch [35/35], Emotion Loss: 1.8744\n",
            "Epoch [42/50], Average Emotion Loss: 1.8677\n",
            "Epoch [43/50], Batch [5/35], Emotion Loss: 1.8836\n",
            "Epoch [43/50], Batch [10/35], Emotion Loss: 1.8256\n",
            "Epoch [43/50], Batch [15/35], Emotion Loss: 1.9100\n",
            "Epoch [43/50], Batch [20/35], Emotion Loss: 1.8972\n",
            "Epoch [43/50], Batch [25/35], Emotion Loss: 1.8921\n",
            "Epoch [43/50], Batch [30/35], Emotion Loss: 1.8702\n",
            "Epoch [43/50], Batch [35/35], Emotion Loss: 1.8375\n",
            "Epoch [43/50], Average Emotion Loss: 1.8656\n",
            "Epoch [44/50], Batch [5/35], Emotion Loss: 1.8512\n",
            "Epoch [44/50], Batch [10/35], Emotion Loss: 1.9410\n",
            "Epoch [44/50], Batch [15/35], Emotion Loss: 1.8800\n",
            "Epoch [44/50], Batch [20/35], Emotion Loss: 1.8692\n",
            "Epoch [44/50], Batch [25/35], Emotion Loss: 1.7922\n",
            "Epoch [44/50], Batch [30/35], Emotion Loss: 1.8889\n",
            "Epoch [44/50], Batch [35/35], Emotion Loss: 1.8466\n",
            "Epoch [44/50], Average Emotion Loss: 1.8648\n",
            "Epoch [45/50], Batch [5/35], Emotion Loss: 1.8840\n",
            "Epoch [45/50], Batch [10/35], Emotion Loss: 1.9149\n",
            "Epoch [45/50], Batch [15/35], Emotion Loss: 1.8940\n",
            "Epoch [45/50], Batch [20/35], Emotion Loss: 1.8489\n",
            "Epoch [45/50], Batch [25/35], Emotion Loss: 1.8355\n",
            "Epoch [45/50], Batch [30/35], Emotion Loss: 1.8543\n",
            "Epoch [45/50], Batch [35/35], Emotion Loss: 1.8778\n",
            "Epoch [45/50], Average Emotion Loss: 1.8595\n",
            "Epoch [46/50], Batch [5/35], Emotion Loss: 1.8341\n",
            "Epoch [46/50], Batch [10/35], Emotion Loss: 1.8350\n",
            "Epoch [46/50], Batch [15/35], Emotion Loss: 1.8568\n",
            "Epoch [46/50], Batch [20/35], Emotion Loss: 1.7893\n",
            "Epoch [46/50], Batch [25/35], Emotion Loss: 1.9331\n",
            "Epoch [46/50], Batch [30/35], Emotion Loss: 1.8314\n",
            "Epoch [46/50], Batch [35/35], Emotion Loss: 1.8652\n",
            "Epoch [46/50], Average Emotion Loss: 1.8536\n",
            "Epoch [47/50], Batch [5/35], Emotion Loss: 1.8276\n",
            "Epoch [47/50], Batch [10/35], Emotion Loss: 1.8275\n",
            "Epoch [47/50], Batch [15/35], Emotion Loss: 1.8264\n",
            "Epoch [47/50], Batch [20/35], Emotion Loss: 1.8444\n",
            "Epoch [47/50], Batch [25/35], Emotion Loss: 1.8822\n",
            "Epoch [47/50], Batch [30/35], Emotion Loss: 1.8718\n",
            "Epoch [47/50], Batch [35/35], Emotion Loss: 1.8107\n",
            "Epoch [47/50], Average Emotion Loss: 1.8531\n",
            "Epoch [48/50], Batch [5/35], Emotion Loss: 1.8011\n",
            "Epoch [48/50], Batch [10/35], Emotion Loss: 1.8731\n",
            "Epoch [48/50], Batch [15/35], Emotion Loss: 1.9101\n",
            "Epoch [48/50], Batch [20/35], Emotion Loss: 1.8508\n",
            "Epoch [48/50], Batch [25/35], Emotion Loss: 1.8193\n",
            "Epoch [48/50], Batch [30/35], Emotion Loss: 1.8663\n",
            "Epoch [48/50], Batch [35/35], Emotion Loss: 1.8276\n",
            "Epoch [48/50], Average Emotion Loss: 1.8468\n",
            "Epoch [49/50], Batch [5/35], Emotion Loss: 1.8454\n",
            "Epoch [49/50], Batch [10/35], Emotion Loss: 1.8932\n",
            "Epoch [49/50], Batch [15/35], Emotion Loss: 1.8535\n",
            "Epoch [49/50], Batch [20/35], Emotion Loss: 1.8104\n",
            "Epoch [49/50], Batch [25/35], Emotion Loss: 1.8199\n",
            "Epoch [49/50], Batch [30/35], Emotion Loss: 1.7965\n",
            "Epoch [49/50], Batch [35/35], Emotion Loss: 1.8369\n",
            "Epoch [49/50], Average Emotion Loss: 1.8407\n",
            "Epoch [50/50], Batch [5/35], Emotion Loss: 1.8525\n",
            "Epoch [50/50], Batch [10/35], Emotion Loss: 1.8487\n",
            "Epoch [50/50], Batch [15/35], Emotion Loss: 1.9192\n",
            "Epoch [50/50], Batch [20/35], Emotion Loss: 1.8020\n",
            "Epoch [50/50], Batch [25/35], Emotion Loss: 1.7935\n",
            "Epoch [50/50], Batch [30/35], Emotion Loss: 1.8703\n",
            "Epoch [50/50], Batch [35/35], Emotion Loss: 1.7920\n",
            "Epoch [50/50], Average Emotion Loss: 1.8417\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Parent directory /project/msoleyma_1026/ecp/models/emotion/late does not exist.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 59\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Batch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_idx\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataloader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     55\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEmotion Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_emotion\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Average Emotion Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_loss_emotion\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(dataloader)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 59\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/project/msoleyma_1026/ecp/models/emotion/late/emotion_detection_model_late_audio.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/project/msoleyma_1026/ecp/ecp-venv/lib/python3.12/site-packages/torch/serialization.py:628\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    625\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 628\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    629\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[1;32m    630\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
            "File \u001b[0;32m/project/msoleyma_1026/ecp/ecp-venv/lib/python3.12/site-packages/torch/serialization.py:502\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    501\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[0;32m--> 502\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/project/msoleyma_1026/ecp/ecp-venv/lib/python3.12/site-packages/torch/serialization.py:473\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mPyTorchFileWriter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream))\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 473\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Parent directory /project/msoleyma_1026/ecp/models/emotion/late does not exist."
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"Cleared CUDA cache\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "data_dir = \"/project/msoleyma_1026/ecp/data/train.json\"\n",
        "embeddings_dir = \"/project/msoleyma_1026/ecp/data/audio/train-emotion/\"\n",
        "num_emotions = 7  # Number of emotions for classification\n",
        "input_dim = 1024  # Dimensionality of your embeddings\n",
        "hidden_dim = 512  # Hidden dimension for the Transformer\n",
        "n_layers = 4  # Number of layers in the Transformer\n",
        "dropout = 0  # Dropout probability\n",
        "batch_size = 32\n",
        "num_epochs = 50\n",
        "learning_rate = 1e-5\n",
        "print_interval = 5\n",
        "\n",
        "dataset = ConversationDataset(data_dir, embeddings_dir)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "class_weights = dataset.class_weights.to(device)\n",
        "print(class_weights)\n",
        "\n",
        "model = EmotionDetector(input_dim, hidden_dim, num_emotions, n_layers=n_layers, dropout=dropout, class_weights=class_weights).to(device)\n",
        "\n",
        "criterion_emotion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss_emotion = 0.0\n",
        "    for batch_idx, (context_embeddings, emotions, convID, utteranceList, labelsList) in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        context_embeddings, emotions = context_embeddings.to(device), emotions.to(device)\n",
        "        prediction_emotion_logits = model(context_embeddings)\n",
        "        \n",
        "        outputs_reshaped = prediction_emotion_logits.view(-1, num_emotions)\n",
        "        emotions_reshaped = emotions.view(-1)\n",
        "\n",
        "        # Compute the loss only on non-padded data points\n",
        "        active_outputs = outputs_reshaped[emotions_reshaped != -1]\n",
        "        active_emotions = emotions_reshaped[emotions_reshaped != -1]\n",
        "        loss_emotion = criterion_emotion(active_outputs, active_emotions)\n",
        "        \n",
        "        loss_emotion.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss_emotion += loss_emotion.item()\n",
        "\n",
        "        if (batch_idx + 1) % print_interval == 0:\n",
        "            print(f'Epoch [{epoch + 1}/{num_epochs}], Batch [{batch_idx + 1}/{len(dataloader)}], '\n",
        "                  f'Emotion Loss: {loss_emotion.item():.4f}')\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}], Average Emotion Loss: {epoch_loss_emotion / len(dataloader):.4f}')\n",
        "\n",
        "torch.save(model.state_dict(), '/project/msoleyma_1026/ecp/models/emotion/late/emotion_detection_model_late_audio.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6-ogDmuZ6Yz",
        "outputId": "45f9babe-292c-4cf6-c1e0-e627982f7ac0"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, dataloader, criterion, device, num_emotions):\n",
        "    \"\"\"\n",
        "    Evaluate the model on the given dataloader and compute loss and accuracy, accounting for padding.\n",
        "\n",
        "    Args:\n",
        "    - model (torch.nn.Module): The model to evaluate.\n",
        "    - dataloader (DataLoader): The DataLoader providing the dataset.\n",
        "    - criterion (loss function): The loss function used to evaluate the model's performance.\n",
        "    - device (torch.device): The device computations will be performed on.\n",
        "    - num_emotions (int): Number of emotion categories used in the model output.\n",
        "\n",
        "    Returns:\n",
        "    - float: Average loss over the dataset.\n",
        "    - float: Accuracy, excluding padded data points.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_valid = 0  # Total non-padded data points\n",
        "    all_predictions = []\n",
        "    all_true_emotions = []\n",
        "    predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            context_embeddings, emotions, conversationList, utteranceList, labelsList = batch\n",
        "            context_embeddings, emotions = context_embeddings.to(device), emotions.to(device)\n",
        "\n",
        "            outputs = model(context_embeddings)\n",
        "\n",
        "            # Reshape for loss calculation\n",
        "            outputs_reshaped = outputs.view(-1, num_emotions)\n",
        "            emotions_reshaped = emotions.view(-1)\n",
        "\n",
        "            # Compute the loss only on non-padded data points\n",
        "            active_outputs = outputs_reshaped[emotions_reshaped != -1]\n",
        "            active_emotions = emotions_reshaped[emotions_reshaped != -1]\n",
        "            loss = criterion(active_outputs, active_emotions)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            _, predicted = torch.max(outputs, dim=2)\n",
        "            valid_data = (emotions != -1)\n",
        "            correct_predictions = predicted.eq(emotions) & valid_data\n",
        "            total_correct += correct_predictions.sum().item()\n",
        "            total_valid += valid_data.sum().item()\n",
        "\n",
        "            all_predictions.extend(predicted[valid_data].cpu().numpy())\n",
        "            all_true_emotions.extend(active_emotions.cpu().numpy())\n",
        "\n",
        "            for i, conv_id in enumerate(conversationList):\n",
        "                conv_data = {'conversation_id': int(conv_id), 'utterances': []}\n",
        "                for j, utterance_tensor in enumerate(utteranceList):\n",
        "                    if int(utterance_tensor[i]) != 0:\n",
        "                        conv_data['utterances'].append({'utt_id': int(utterance_tensor[i]), 'emotion': int(predicted[i][j])})\n",
        "                predictions.append(conv_data)\n",
        "\n",
        "    file_path = \"/project/msoleyma_1026/ecp/data/predictions/late_fusion_predictions_audio_emotion.json\"\n",
        "\n",
        "    with open(file_path, 'w') as file:\n",
        "        json.dump(predictions, file, indent=4)\n",
        "    \n",
        "    f1_score_val = f1_score(all_true_emotions, all_predictions, average='weighted')\n",
        "    classification_report_result = classification_report(all_true_emotions, all_predictions)\n",
        "\n",
        "    average_loss = total_loss / len(dataloader)\n",
        "    accuracy = total_correct / total_valid if total_valid > 0 else 0\n",
        "    \n",
        "    return average_loss, accuracy, f1_score_val, classification_report_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss: 1.9511, Validation Accuracy: 0.1247%\n",
            "F1 Score: 11.00%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.22      0.23      0.22       333\n",
            "           1       0.05      0.04      0.04        79\n",
            "           2       0.03      0.52      0.05        56\n",
            "           3       0.22      0.11      0.15       429\n",
            "           4       0.57      0.01      0.01      1121\n",
            "           5       0.16      0.35      0.22       241\n",
            "           6       0.22      0.23      0.23       307\n",
            "\n",
            "    accuracy                           0.12      2566\n",
            "   macro avg       0.21      0.21      0.13      2566\n",
            "weighted avg       0.36      0.12      0.11      2566\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data_dir_test = \"/project/msoleyma_1026/ecp/data/test.json\"\n",
        "embeddings_dir_test = \"/project/msoleyma_1026/ecp/data/audio/test-emotion/\"\n",
        "\n",
        "dataset = ConversationDataset(data_dir_test, embeddings_dir_test)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "validation_dataloader = DataLoader(dataset, batch_size=8, shuffle=False)  # Same dataloader for simplicity\n",
        "model.to(device)\n",
        "criterion_emotion = nn.CrossEntropyLoss()\n",
        "\n",
        "val_loss, val_accuracy, val_f1, val_class_report = evaluate_model(model, validation_dataloader, criterion_emotion, device, num_emotions=7)\n",
        "print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}%\")\n",
        "print(f\"F1 Score: {val_f1 * 100:.2f}%\")\n",
        "print(val_class_report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
