{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23021,"status":"ok","timestamp":1713730332673,"user":{"displayName":"Anuranjan Pandey","userId":"12921146263473008416"},"user_tz":420},"id":"ant_mxKxkZP9","outputId":"531593c5-95ec-45e1-b807-56ff09c987cd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5872,"status":"ok","timestamp":1713310525257,"user":{"displayName":"Kriti Jha","userId":"09845339065765737656"},"user_tz":420},"id":"sQgNYU5ub1qg","outputId":"68161ef4-9f70-4565-d6cb-6c6b5fbd6bbb"},"outputs":[{"data":{"text/plain":["(1, 2560)"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np\n","x = np.load(\"/content/drive/MyDrive/CSCI535 Project/Dataset/Processed/concatenated_data/dia1001utt10_concatenated.npy\")\n","x.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":237,"status":"ok","timestamp":1713310525477,"user":{"displayName":"Kriti Jha","userId":"09845339065765737656"},"user_tz":420},"id":"WWi29yesQ_VI","outputId":"65a71edd-049e-4cce-bc59-3af97e10d248"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'dia530utt12': array([[-0.02330273, -0.1057745 , -0.02929482, ..., -0.13061267,\n","        -0.17919499,  0.12389535]], dtype=float32)}\n"]}],"source":["import os\n","import numpy as np\n","import json\n","\n","# Step 1: Load Embeddings\n","\n","def load_embeddings_from_directory(embeddings_dir):\n","    embeddings = {}\n","    for filename in os.listdir(embeddings_dir):\n","        if filename.endswith(\".npy\"):\n","            utterance_id = os.path.splitext(filename)[0]\n","            utterance_id = utterance_id.split('_')[0]\n","            # print(utterance_id)\n","            # break\n","            embedding_path = os.path.join(embeddings_dir, filename)\n","            embedding = np.load(embedding_path)\n","            embeddings[utterance_id] = embedding\n","            break\n","    return embeddings\n","\n","# Example usage\n","embeddings_dir = '/content/drive/MyDrive/CSCI535 Project/Dataset/Processed/concatenated_data'\n","embeddings = load_embeddings_from_directory(embeddings_dir)\n","print(embeddings)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":126987,"status":"ok","timestamp":1713314329012,"user":{"displayName":"Kriti Jha","userId":"09845339065765737656"},"user_tz":420},"id":"-ebSo_3zAfqz","outputId":"ba8ef3b9-dcaa-4812-d443-5985eb4c9b4f"},"outputs":[{"name":"stdout","output_type":"stream","text":["1.8218151330947876\n","2.305945634841919\n","1.8957395553588867\n","1.494415521621704\n","Epoch [1/30], Average Emotion Loss: 1.8795\n","0.704918622970581\n","1.7075051069259644\n","0.7416472434997559\n","0.9643366932868958\n","Epoch [2/30], Average Emotion Loss: 1.0296\n","0.839963436126709\n","0.9937276244163513\n","1.0661314725875854\n","1.0811163187026978\n","Epoch [3/30], Average Emotion Loss: 0.9952\n","1.0471463203430176\n","0.8017507195472717\n","0.7013596296310425\n","0.5909005999565125\n","Epoch [4/30], Average Emotion Loss: 0.7853\n","0.6465285420417786\n","0.737175703048706\n","0.7775102257728577\n","0.7051157355308533\n","Epoch [5/30], Average Emotion Loss: 0.7166\n","0.712006688117981\n","0.7361847162246704\n","0.6434614062309265\n","0.7474231719970703\n","Epoch [6/30], Average Emotion Loss: 0.7098\n","0.6166574358940125\n","0.6047108173370361\n","0.6500586271286011\n","0.5543012619018555\n","Epoch [7/30], Average Emotion Loss: 0.6064\n","0.5282276272773743\n","0.597628653049469\n","0.5601637959480286\n","0.4945029616355896\n","Epoch [8/30], Average Emotion Loss: 0.5451\n","0.5569114089012146\n","0.614231526851654\n","0.41250374913215637\n","0.5346398949623108\n","Epoch [9/30], Average Emotion Loss: 0.5296\n","0.5918399691581726\n","0.4852651059627533\n","0.46678805351257324\n","0.4536602795124054\n","Epoch [10/30], Average Emotion Loss: 0.4994\n","0.5097856521606445\n","0.49340635538101196\n","0.45269444584846497\n","0.6444041132926941\n","Epoch [11/30], Average Emotion Loss: 0.5251\n","0.47120901942253113\n","0.5224801301956177\n","0.5327590703964233\n","0.5188841223716736\n","Epoch [12/30], Average Emotion Loss: 0.5113\n","0.5374311804771423\n","0.5769489407539368\n","0.453212171792984\n","0.4200674891471863\n","Epoch [13/30], Average Emotion Loss: 0.4969\n","0.521296501159668\n","0.49848654866218567\n","0.4565682113170624\n","0.5693084597587585\n","Epoch [14/30], Average Emotion Loss: 0.5114\n","0.518256425857544\n","0.5195940732955933\n","0.4862769842147827\n","0.48271945118904114\n","Epoch [15/30], Average Emotion Loss: 0.5017\n","0.5134615302085876\n","0.5137418508529663\n","0.4767283797264099\n","0.4854956567287445\n","Epoch [16/30], Average Emotion Loss: 0.4974\n","0.530793309211731\n","0.42818209528923035\n","0.5461897253990173\n","0.4893104135990143\n","Epoch [17/30], Average Emotion Loss: 0.4986\n","0.5008788704872131\n","0.5728505253791809\n","0.4133055806159973\n","0.48275482654571533\n","Epoch [18/30], Average Emotion Loss: 0.4924\n","0.49343737959861755\n","0.47664597630500793\n","0.4959902763366699\n","0.5072981119155884\n","Epoch [19/30], Average Emotion Loss: 0.4933\n","0.515336275100708\n","0.43102702498435974\n","0.4832337200641632\n","0.5973693132400513\n","Epoch [20/30], Average Emotion Loss: 0.5067\n","0.52089524269104\n","0.4658154547214508\n","0.4183974266052246\n","0.6104146242141724\n","Epoch [21/30], Average Emotion Loss: 0.5039\n","0.49300453066825867\n","0.45124921202659607\n","0.4405077397823334\n","0.5818769931793213\n","Epoch [22/30], Average Emotion Loss: 0.4917\n","0.41589128971099854\n","0.4368663728237152\n","0.5459747314453125\n","0.4879554510116577\n","Epoch [23/30], Average Emotion Loss: 0.4717\n","0.4880579710006714\n","0.40635544061660767\n","0.44515180587768555\n","0.6176593899726868\n","Epoch [24/30], Average Emotion Loss: 0.4893\n","0.49613022804260254\n","0.49822473526000977\n","0.5166298151016235\n","0.4856371581554413\n","Epoch [25/30], Average Emotion Loss: 0.4992\n","0.49712151288986206\n","0.5213791728019714\n","0.48291510343551636\n","0.4141860008239746\n","Epoch [26/30], Average Emotion Loss: 0.4789\n","0.4232706129550934\n","0.540176510810852\n","0.5194533467292786\n","0.4399079382419586\n","Epoch [27/30], Average Emotion Loss: 0.4807\n","0.4475439786911011\n","0.5266097187995911\n","0.3750247359275818\n","0.7163479924201965\n","Epoch [28/30], Average Emotion Loss: 0.5164\n","0.48612865805625916\n","0.45358145236968994\n","0.43129968643188477\n","0.5481167435646057\n","Epoch [29/30], Average Emotion Loss: 0.4798\n","0.5112846493721008\n","0.43861258029937744\n","0.36301177740097046\n","0.5169302225112915\n","Epoch [30/30], Average Emotion Loss: 0.4575\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","from torch.nn import TransformerEncoder, TransformerEncoderLayer\n","import torch.nn.functional as F\n","from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n","\n","class ConversationDataset(Dataset):\n","    def __init__(self, data_dir, embeddings_dir):\n","        self.data_dir = data_dir\n","        self.embeddings_dir = embeddings_dir\n","        self.conversations = self.load_conversations()\n","        self.emotion_list = sorted(['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness','surprise'])\n","        self.encoder = LabelEncoder()\n","        self.encoder.fit(self.emotion_list)\n","\n","    def encode_emotion(self, emotion):\n","        return self.encoder.transform([emotion])[0]\n","\n","    def load_conversations(self):\n","        with open(self.data_dir, 'r') as file:\n","            conversations = json.load(file)\n","        return conversations\n","\n","    def load_embeddings(self, video_name):\n","        video_name = video_name.split('.')[0] + '_concatenated'\n","\n","        embedding_file = os.path.join(self.embeddings_dir, f'{video_name}.npy')\n","        embedding = np.load(embedding_file)\n","        return embedding\n","\n","    def positional_encoding(self, embeddings):\n","        seq_length = embeddings.shape[0]\n","        embedding_dim = embeddings.shape[1]\n","        position_enc = torch.zeros(seq_length, embedding_dim)\n","        position = torch.arange(0, seq_length, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-np.log(10000.0) / embedding_dim))\n","        position_enc[:, 0::2] = torch.sin(position * div_term)\n","        position_enc[:, 1::2] = torch.cos(position * div_term)\n","        # print(position_enc.shape)\n","        return position_enc\n","\n","    def __len__(self):\n","        return len(self.conversations)\n","\n","    def __getitem__(self, idx):\n","        conversation = self.conversations[idx]\n","        context_embeddings = []\n","        emotions = []\n","\n","        for utterance in conversation['conversation']:\n","            video_name = utterance['video_name']\n","            embedding = self.load_embeddings(video_name)\n","            context_embeddings.append(torch.from_numpy(embedding))\n","            emotions.append(self.encode_emotion(utterance['emotion']))\n","\n","        max_seq_length = 33\n","        padded_embeddings = []\n","        if len(context_embeddings) < max_seq_length:\n","            num_to_add = max_seq_length - len(context_embeddings)\n","            zero_tensor = torch.zeros((1,2560), dtype=torch.float32)\n","            context_embeddings += [zero_tensor] * num_to_add\n","\n","        context_embeddings_padded = torch.cat(context_embeddings, dim=0)\n","\n","        positional_encodings = self.positional_encoding(context_embeddings_padded)\n","        context_embeddings_with_pos = context_embeddings_padded + positional_encodings\n","\n","        emotions += [self.encode_emotion('neutral')] * num_to_add\n","        encoded_emotions_tensor = torch.tensor(emotions, dtype=torch.long)\n","\n","        return context_embeddings_with_pos, encoded_emotions_tensor\n","\n","class EmotionDetector(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_emotions, n_layers=2, dropout=0.2):\n","        super(EmotionDetector, self).__init__()\n","        self.input_dim = input_dim\n","        self.hidden_dim = hidden_dim\n","        self.num_emotions = num_emotions\n","        self.n_layers = n_layers\n","\n","        encoder_layers = TransformerEncoderLayer(d_model=input_dim, nhead=4, dim_feedforward=hidden_dim, dropout=dropout, batch_first=True)\n","        self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers=n_layers)\n","\n","        self.decoder_emotion = nn.Linear(input_dim, num_emotions)\n","\n","    def forward(self, context_embeddings):\n","        encoded_context = self.transformer_encoder(context_embeddings)\n","        prediction_emotion = self.decoder_emotion(encoded_context)  # (batch_size, seq_length, num_emotions)\n","        return prediction_emotion\n","\n","if torch.cuda.is_available():\n","    torch.cuda.empty_cache()\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","data_dir = \"/content/drive/MyDrive/CSCI535 Project/Dataset/text/dev.json\"\n","embeddings_dir = \"/content/drive/MyDrive/CSCI535 Project/Dataset/Processed/concatenated_data\"\n","num_emotions = 7  # Number of emotions for classification\n","input_dim = 2560  # Dimensionality of your embeddings\n","hidden_dim = 512  # Hidden dimension for the Transformer\n","n_layers = 3  # Number of layers in the Transformer\n","dropout = 0.25  # Dropout probability\n","batch_size = 32\n","num_epochs = 30\n","learning_rate = 0.0001\n","print_interval = 5\n","\n","dataset = ConversationDataset(data_dir, embeddings_dir)\n","dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","\n","model = EmotionDetector(input_dim, hidden_dim, num_emotions, n_layers=n_layers, dropout=dropout).to(device)\n","\n","criterion_emotion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    epoch_loss_emotion = 0.0\n","    for batch_idx, (context_embeddings, emotions) in enumerate(dataloader):\n","\n","        optimizer.zero_grad()\n","        context_embeddings, emotions = context_embeddings.to(device), emotions.to(device)\n","        prediction_emotion_logits = model(context_embeddings)\n","        loss_emotion = criterion_emotion(prediction_emotion_logits.view(-1, num_emotions), emotions.view(-1))\n","\n","        loss_emotion.backward()\n","        optimizer.step()\n","\n","        epoch_loss_emotion += loss_emotion.item()\n","        print(loss_emotion.item())\n","\n","        if (batch_idx + 1) % print_interval == 0:\n","            print(f'Epoch [{epoch + 1}/{num_epochs}], Batch [{batch_idx + 1}/{len(dataloader)}], '\n","                  f'Emotion Loss: {loss_emotion.item():.4f}')\n","\n","    print(f'Epoch [{epoch + 1}/{num_epochs}], Average Emotion Loss: {epoch_loss_emotion / len(dataloader):.4f}')\n","\n","torch.save(model.state_dict(), 'emotion_detection_model.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":237,"status":"ok","timestamp":1713310618395,"user":{"displayName":"Kriti Jha","userId":"09845339065765737656"},"user_tz":420},"id":"yiQuxiYSiVrm","outputId":"893635e4-683c-4ba5-e8ea-9e63ece34610"},"outputs":[{"data":{"text/plain":["tensor([[0.0719, 0.1026, 0.5198, 0.2240, 0.0817],\n","        [0.1155, 0.3262, 0.1952, 0.1305, 0.2327],\n","        [0.4642, 0.1308, 0.2186, 0.0577, 0.1287]])"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["test = torch.randn(3, 5).softmax(dim=1)\n","test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4oWAEBsBQuY_"},"outputs":[],"source":["def evaluate_model(model, dataloader, criterion):\n","    model.eval()\n","    total_loss = 0.0\n","    total_correct = 0\n","    total_samples = 0\n","\n","    with torch.no_grad():\n","        for batch in dataloader:\n","            context_embeddings, target_emotions = batch\n","            output = model(context_embeddings)\n","            loss = criterion(output, target_emotions)\n","            total_loss += loss.item()\n","\n","            _, predicted_emotions = torch.max(output, 1)\n","            total_correct += (predicted_emotions == target_emotions).sum().item()\n","            total_samples += target_emotions.size(0)\n","\n","    average_loss = total_loss / len(dataloader)\n","    accuracy = total_correct / total_samples\n","\n","    return average_loss, accuracy\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":216},"executionInfo":{"elapsed":510,"status":"error","timestamp":1713310621885,"user":{"displayName":"Kriti Jha","userId":"09845339065765737656"},"user_tz":420},"id":"7bqEY_OnQxo3","outputId":"5d870e9f-35e2-4848-cd21-adb22e851f1d"},"outputs":[],"source":["# Example usage\n","validation_dataloader = DataLoader(validation_dataset, batch_size=64, shuffle=False)\n","\n","val_loss, val_accuracy = evaluate_model(model, validation_dataloader, criterion)\n","print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"50GYHjIaRV0q"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
